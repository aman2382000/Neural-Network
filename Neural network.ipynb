{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch import optim\n",
    "import matplotlib as mpl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\Anaconda\\lib\\site-packages\\sklearn\\preprocessing\\data.py:645: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\Users\\Dell\\Anaconda\\lib\\site-packages\\sklearn\\base.py:464: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('data.csv')\n",
    "sc = StandardScaler()\n",
    "features = df[[\"CNT\",\"Si3N4\",\"SPEED\",\"DEPTH\"]]\n",
    "df_new = sc.fit_transform(features)\n",
    "#df_new = pd.DataFrame(df_new)\n",
    "x_train, x_test, y_train, y_test = train_test_split(df_new, df[[\"TENSILE\"]], test_size=0.15, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.from_numpy(x_train)\n",
    "y_train = torch.from_numpy(y_train.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 10941.43125\n",
      "Training loss: 10935.09765625\n",
      "Training loss: 10928.8734375\n",
      "Training loss: 10922.64375\n",
      "Training loss: 10916.29765625\n",
      "Training loss: 10909.71875\n",
      "Training loss: 10902.77890625\n",
      "Training loss: 10895.31875\n",
      "Training loss: 10887.1484375\n",
      "Training loss: 10878.0171875\n",
      "Training loss: 10867.5953125\n",
      "Training loss: 10855.42890625\n",
      "Training loss: 10840.87890625\n",
      "Training loss: 10823.01875\n",
      "Training loss: 10800.4578125\n",
      "Training loss: 10771.03984375\n",
      "Training loss: 10731.2765625\n",
      "Training loss: 10675.26015625\n",
      "Training loss: 10592.4390625\n",
      "Training loss: 10462.834375\n",
      "Training loss: 10245.97734375\n",
      "Training loss: 9853.83515625\n",
      "Training loss: 9082.02890625\n",
      "Training loss: 7450.240625\n",
      "Training loss: 4108.325390625\n",
      "Training loss: 224.274365234375\n",
      "Training loss: 161.92723388671874\n",
      "Training loss: 243.16875\n",
      "Training loss: 184.1881103515625\n",
      "Training loss: 287.592236328125\n",
      "Training loss: 205.8074951171875\n",
      "Training loss: 325.425732421875\n",
      "Training loss: 222.007958984375\n",
      "Training loss: 353.624072265625\n",
      "Training loss: 232.8587646484375\n",
      "Training loss: 372.490087890625\n",
      "Training loss: 239.5067626953125\n",
      "Training loss: 383.9869140625\n",
      "Training loss: 243.2510498046875\n",
      "Training loss: 390.3667236328125\n",
      "Training loss: 245.1415283203125\n",
      "Training loss: 393.4702392578125\n",
      "Training loss: 245.9035400390625\n",
      "Training loss: 394.577587890625\n",
      "Training loss: 245.9972412109375\n",
      "Training loss: 394.5034423828125\n",
      "Training loss: 245.7010986328125\n",
      "Training loss: 393.740576171875\n",
      "Training loss: 245.1796630859375\n",
      "Training loss: 392.5827392578125\n",
      "Training loss: 244.529443359375\n",
      "Training loss: 391.1996826171875\n",
      "Training loss: 243.8059326171875\n",
      "Training loss: 389.690234375\n",
      "Training loss: 243.040966796875\n",
      "Training loss: 388.1107421875\n",
      "Training loss: 242.2533935546875\n",
      "Training loss: 386.4946044921875\n",
      "Training loss: 241.454248046875\n",
      "Training loss: 384.8610107421875\n",
      "Training loss: 240.6484375\n",
      "Training loss: 383.21875\n",
      "Training loss: 239.84013671875\n",
      "Training loss: 381.575439453125\n",
      "Training loss: 239.03232421875\n",
      "Training loss: 379.9360595703125\n",
      "Training loss: 238.224951171875\n",
      "Training loss: 378.3001220703125\n",
      "Training loss: 237.4191650390625\n",
      "Training loss: 376.6697998046875\n",
      "Training loss: 236.6152099609375\n",
      "Training loss: 375.046240234375\n",
      "Training loss: 235.8134033203125\n",
      "Training loss: 373.429248046875\n",
      "Training loss: 235.0134521484375\n",
      "Training loss: 371.8184326171875\n",
      "Training loss: 234.216064453125\n",
      "Training loss: 370.214697265625\n",
      "Training loss: 233.420703125\n",
      "Training loss: 368.6178955078125\n",
      "Training loss: 232.627783203125\n",
      "Training loss: 367.02734375\n",
      "Training loss: 231.836767578125\n",
      "Training loss: 365.44375\n",
      "Training loss: 231.048486328125\n",
      "Training loss: 363.867431640625\n",
      "Training loss: 230.262255859375\n",
      "Training loss: 362.297412109375\n",
      "Training loss: 229.477978515625\n",
      "Training loss: 360.7337890625\n",
      "Training loss: 228.6958984375\n",
      "Training loss: 359.1760498046875\n",
      "Training loss: 227.9158203125\n",
      "Training loss: 357.6251953125\n",
      "Training loss: 227.1384033203125\n",
      "Training loss: 356.081494140625\n",
      "Training loss: 226.3628662109375\n",
      "Training loss: 354.543408203125\n",
      "Training loss: 225.589111328125\n",
      "Training loss: 353.0115234375\n",
      "Training loss: 224.818017578125\n",
      "Training loss: 351.486474609375\n",
      "Training loss: 224.048486328125\n",
      "Training loss: 349.9666015625\n",
      "Training loss: 223.2805908203125\n",
      "Training loss: 348.4522705078125\n",
      "Training loss: 222.5148681640625\n",
      "Training loss: 346.944873046875\n",
      "Training loss: 221.7517578125\n",
      "Training loss: 345.4436279296875\n",
      "Training loss: 220.9899658203125\n",
      "Training loss: 343.9474609375\n",
      "Training loss: 220.2298828125\n",
      "Training loss: 342.4571533203125\n",
      "Training loss: 219.472314453125\n",
      "Training loss: 340.973583984375\n",
      "Training loss: 218.71640625\n",
      "Training loss: 339.494677734375\n",
      "Training loss: 217.962158203125\n",
      "Training loss: 338.0219482421875\n",
      "Training loss: 217.2100341796875\n",
      "Training loss: 336.5548828125\n",
      "Training loss: 216.4598876953125\n",
      "Training loss: 335.093798828125\n",
      "Training loss: 215.71162109375\n",
      "Training loss: 333.638525390625\n",
      "Training loss: 214.9657470703125\n",
      "Training loss: 332.1895751953125\n",
      "Training loss: 214.2213623046875\n",
      "Training loss: 330.7453857421875\n",
      "Training loss: 213.4786376953125\n",
      "Training loss: 329.30625\n",
      "Training loss: 212.7377197265625\n",
      "Training loss: 327.873046875\n",
      "Training loss: 211.998681640625\n",
      "Training loss: 326.445361328125\n",
      "Training loss: 211.261669921875\n",
      "Training loss: 325.023046875\n",
      "Training loss: 210.5262451171875\n",
      "Training loss: 323.606005859375\n",
      "Training loss: 209.7925048828125\n",
      "Training loss: 322.1945068359375\n",
      "Training loss: 209.061279296875\n",
      "Training loss: 320.7893798828125\n",
      "Training loss: 208.331494140625\n",
      "Training loss: 319.388671875\n",
      "Training loss: 207.603759765625\n",
      "Training loss: 317.994091796875\n",
      "Training loss: 206.877880859375\n",
      "Training loss: 316.6045654296875\n",
      "Training loss: 206.1531982421875\n",
      "Training loss: 315.219873046875\n",
      "Training loss: 205.4309326171875\n",
      "Training loss: 313.8412109375\n",
      "Training loss: 204.71019287109374\n",
      "Training loss: 312.4669189453125\n",
      "Training loss: 203.99117431640624\n",
      "Training loss: 311.0983642578125\n",
      "Training loss: 203.27418212890626\n",
      "Training loss: 309.735302734375\n",
      "Training loss: 202.55885009765626\n",
      "Training loss: 308.377001953125\n",
      "Training loss: 201.8454345703125\n",
      "Training loss: 307.0241455078125\n",
      "Training loss: 201.1336669921875\n",
      "Training loss: 305.6765869140625\n",
      "Training loss: 200.4240478515625\n",
      "Training loss: 304.334716796875\n",
      "Training loss: 199.7163818359375\n",
      "Training loss: 302.9979248046875\n",
      "Training loss: 199.00986328125\n",
      "Training loss: 301.665576171875\n",
      "Training loss: 198.3053955078125\n",
      "Training loss: 300.3384765625\n",
      "Training loss: 197.60306396484376\n",
      "Training loss: 299.017041015625\n",
      "Training loss: 196.90191650390625\n",
      "Training loss: 297.6998046875\n",
      "Training loss: 196.20269775390625\n",
      "Training loss: 296.387939453125\n",
      "Training loss: 195.50560302734374\n",
      "Training loss: 295.0816162109375\n",
      "Training loss: 194.8098876953125\n",
      "Training loss: 293.7795654296875\n",
      "Training loss: 194.11556396484374\n",
      "Training loss: 292.482177734375\n",
      "Training loss: 193.42366943359374\n",
      "Training loss: 291.19072265625\n",
      "Training loss: 192.73345947265625\n",
      "Training loss: 289.9041748046875\n",
      "Training loss: 192.04482421875\n",
      "Training loss: 288.622509765625\n",
      "Training loss: 191.3583740234375\n",
      "Training loss: 287.345703125\n",
      "Training loss: 190.67330322265624\n",
      "Training loss: 286.073828125\n",
      "Training loss: 189.99010009765624\n",
      "Training loss: 284.806884765625\n",
      "Training loss: 189.30865478515625\n",
      "Training loss: 283.5450927734375\n",
      "Training loss: 188.62919921875\n",
      "Training loss: 282.28828125\n",
      "Training loss: 187.95120849609376\n",
      "Training loss: 281.0357666015625\n",
      "Training loss: 187.27523193359374\n",
      "Training loss: 279.7886962890625\n",
      "Training loss: 186.6010009765625\n",
      "Training loss: 278.546533203125\n",
      "Training loss: 185.92867431640624\n",
      "Training loss: 277.3090576171875\n",
      "Training loss: 185.2576904296875\n",
      "Training loss: 276.0759765625\n",
      "Training loss: 184.58873291015624\n",
      "Training loss: 274.847998046875\n",
      "Training loss: 183.92119140625\n",
      "Training loss: 273.624365234375\n",
      "Training loss: 183.2553466796875\n",
      "Training loss: 272.4053955078125\n",
      "Training loss: 182.59166259765624\n",
      "Training loss: 271.1921630859375\n",
      "Training loss: 181.9299072265625\n",
      "Training loss: 269.9837158203125\n",
      "Training loss: 181.26982421875\n",
      "Training loss: 268.779541015625\n",
      "Training loss: 180.6111572265625\n",
      "Training loss: 267.5798583984375\n",
      "Training loss: 179.9543212890625\n",
      "Training loss: 266.3849609375\n",
      "Training loss: 179.29921875\n",
      "Training loss: 265.1947265625\n",
      "Training loss: 178.64586181640624\n",
      "Training loss: 264.009130859375\n",
      "Training loss: 177.9942138671875\n",
      "Training loss: 262.827978515625\n",
      "Training loss: 177.34407958984374\n",
      "Training loss: 261.6513671875\n",
      "Training loss: 176.69613037109374\n",
      "Training loss: 260.4798095703125\n",
      "Training loss: 176.0498291015625\n",
      "Training loss: 259.3129638671875\n",
      "Training loss: 175.40498046875\n",
      "Training loss: 258.150537109375\n",
      "Training loss: 174.76240234375\n",
      "Training loss: 256.9931884765625\n",
      "Training loss: 174.12161865234376\n",
      "Training loss: 255.8405517578125\n",
      "Training loss: 173.48255615234376\n",
      "Training loss: 254.6921875\n",
      "Training loss: 172.84482421875\n",
      "Training loss: 253.54814453125\n",
      "Training loss: 172.2093505859375\n",
      "Training loss: 252.4092529296875\n",
      "Training loss: 171.5751708984375\n",
      "Training loss: 251.274072265625\n",
      "Training loss: 170.942578125\n",
      "Training loss: 250.14345703125\n",
      "Training loss: 170.31201171875\n",
      "Training loss: 249.0177490234375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 169.68316650390625\n",
      "Training loss: 247.896240234375\n",
      "Training loss: 169.0562255859375\n",
      "Training loss: 246.779931640625\n",
      "Training loss: 168.4314697265625\n",
      "Training loss: 245.668408203125\n",
      "Training loss: 167.80771484375\n",
      "Training loss: 244.5601806640625\n",
      "Training loss: 167.18599853515624\n",
      "Training loss: 243.456884765625\n",
      "Training loss: 166.56595458984376\n",
      "Training loss: 242.358056640625\n",
      "Training loss: 165.94716796875\n",
      "Training loss: 241.2628662109375\n",
      "Training loss: 165.33092041015624\n",
      "Training loss: 240.173291015625\n",
      "Training loss: 164.71614990234374\n",
      "Training loss: 239.087744140625\n",
      "Training loss: 164.10279541015626\n",
      "Training loss: 238.006103515625\n",
      "Training loss: 163.4914306640625\n",
      "Training loss: 236.9291259765625\n",
      "Training loss: 162.88201904296875\n",
      "Training loss: 235.8570068359375\n",
      "Training loss: 162.2744140625\n",
      "Training loss: 234.789208984375\n",
      "Training loss: 161.66798095703126\n",
      "Training loss: 233.724951171875\n",
      "Training loss: 161.0635498046875\n",
      "Training loss: 232.6652587890625\n",
      "Training loss: 160.460888671875\n",
      "Training loss: 231.6102294921875\n",
      "Training loss: 159.8600830078125\n",
      "Training loss: 230.5597900390625\n",
      "Training loss: 159.26099853515626\n",
      "Training loss: 229.51328125\n",
      "Training loss: 158.66337890625\n",
      "Training loss: 228.4702880859375\n",
      "Training loss: 158.06732177734375\n",
      "Training loss: 227.431982421875\n",
      "Training loss: 157.4729736328125\n",
      "Training loss: 226.397509765625\n",
      "Training loss: 156.88052978515626\n",
      "Training loss: 225.3677001953125\n",
      "Training loss: 156.28973388671875\n",
      "Training loss: 224.3418212890625\n",
      "Training loss: 155.7007080078125\n",
      "Training loss: 223.320361328125\n",
      "Training loss: 155.1134033203125\n",
      "Training loss: 222.302978515625\n",
      "Training loss: 154.52764892578125\n",
      "Training loss: 221.289990234375\n",
      "Training loss: 153.9441650390625\n",
      "Training loss: 220.281689453125\n",
      "Training loss: 153.362548828125\n",
      "Training loss: 219.277734375\n",
      "Training loss: 152.782080078125\n",
      "Training loss: 218.276806640625\n",
      "Training loss: 152.20328369140626\n",
      "Training loss: 217.2804443359375\n",
      "Training loss: 151.626171875\n",
      "Training loss: 216.2877197265625\n",
      "Training loss: 151.0510009765625\n",
      "Training loss: 215.29970703125\n",
      "Training loss: 150.47740478515624\n",
      "Training loss: 214.315625\n",
      "Training loss: 149.9055908203125\n",
      "Training loss: 213.33564453125\n",
      "Training loss: 149.33558349609376\n",
      "Training loss: 212.359912109375\n",
      "Training loss: 148.76705322265624\n",
      "Training loss: 211.387548828125\n",
      "Training loss: 148.20001220703125\n",
      "Training loss: 210.4194580078125\n",
      "Training loss: 147.6350341796875\n",
      "Training loss: 209.455615234375\n",
      "Training loss: 147.07177734375\n",
      "Training loss: 208.4958740234375\n",
      "Training loss: 146.5103515625\n",
      "Training loss: 207.540283203125\n",
      "Training loss: 145.94993896484374\n",
      "Training loss: 206.587646484375\n",
      "Training loss: 145.3919189453125\n",
      "Training loss: 205.6401611328125\n",
      "Training loss: 144.8350341796875\n",
      "Training loss: 204.69603271484374\n",
      "Training loss: 144.28033447265625\n",
      "Training loss: 203.7562255859375\n",
      "Training loss: 143.7273193359375\n",
      "Training loss: 202.8203857421875\n",
      "Training loss: 143.175830078125\n",
      "Training loss: 201.88822021484376\n",
      "Training loss: 142.6257080078125\n",
      "Training loss: 200.9595703125\n",
      "Training loss: 142.07757568359375\n",
      "Training loss: 200.03509521484375\n",
      "Training loss: 141.53089599609376\n",
      "Training loss: 199.1144287109375\n",
      "Training loss: 140.9857177734375\n",
      "Training loss: 198.19742431640626\n",
      "Training loss: 140.44266357421876\n",
      "Training loss: 197.28447265625\n",
      "Training loss: 139.90086669921874\n",
      "Training loss: 196.37515869140626\n",
      "Training loss: 139.36065673828125\n",
      "Training loss: 195.4695068359375\n",
      "Training loss: 138.822607421875\n",
      "Training loss: 194.56845703125\n",
      "Training loss: 138.2859619140625\n",
      "Training loss: 193.6707763671875\n",
      "Training loss: 137.7511474609375\n",
      "Training loss: 192.77730712890624\n",
      "Training loss: 137.21829833984376\n",
      "Training loss: 191.88775634765625\n",
      "Training loss: 136.6869873046875\n",
      "Training loss: 191.00189208984375\n",
      "Training loss: 136.15699462890626\n",
      "Training loss: 190.1193603515625\n",
      "Training loss: 135.62889404296874\n",
      "Training loss: 189.24075927734376\n",
      "Training loss: 135.1024658203125\n",
      "Training loss: 188.36611328125\n",
      "Training loss: 134.577734375\n",
      "Training loss: 187.4948486328125\n",
      "Training loss: 134.05419921875\n",
      "Training loss: 186.62698974609376\n",
      "Training loss: 133.5323974609375\n",
      "Training loss: 185.7630859375\n",
      "Training loss: 133.0123779296875\n",
      "Training loss: 184.90286865234376\n",
      "Training loss: 132.49404296875\n",
      "Training loss: 184.04609375\n",
      "Training loss: 131.9771484375\n",
      "Training loss: 183.19306640625\n",
      "Training loss: 131.46199951171874\n",
      "Training loss: 182.3438232421875\n",
      "Training loss: 130.9487548828125\n",
      "Training loss: 181.4986572265625\n",
      "Training loss: 130.43701171875\n",
      "Training loss: 180.6567138671875\n",
      "Training loss: 129.92672119140624\n",
      "Training loss: 179.81826171875\n",
      "Training loss: 129.41866455078124\n",
      "Training loss: 178.9843017578125\n",
      "Training loss: 128.9120849609375\n",
      "Training loss: 178.1537353515625\n",
      "Training loss: 128.4069091796875\n",
      "Training loss: 177.32611083984375\n",
      "Training loss: 127.9032958984375\n",
      "Training loss: 176.50228271484374\n",
      "Training loss: 127.40147705078125\n",
      "Training loss: 175.6823486328125\n",
      "Training loss: 126.9012451171875\n",
      "Training loss: 174.86539306640626\n",
      "Training loss: 126.40240478515625\n",
      "Training loss: 174.05208740234374\n",
      "Training loss: 125.9049560546875\n",
      "Training loss: 173.2416015625\n",
      "Training loss: 125.40892333984375\n",
      "Training loss: 172.43453369140624\n",
      "Training loss: 124.91494140625\n",
      "Training loss: 171.63167724609374\n",
      "Training loss: 124.4228515625\n",
      "Training loss: 170.83275146484374\n",
      "Training loss: 123.9321533203125\n",
      "Training loss: 170.03675537109376\n",
      "Training loss: 123.443017578125\n",
      "Training loss: 169.2440185546875\n",
      "Training loss: 122.9553955078125\n",
      "Training loss: 168.45504150390624\n",
      "Training loss: 122.46917724609375\n",
      "Training loss: 167.66881103515624\n",
      "Training loss: 121.98450927734375\n",
      "Training loss: 166.88619384765624\n",
      "Training loss: 121.5014892578125\n",
      "Training loss: 166.10709228515626\n",
      "Training loss: 121.02034912109374\n",
      "Training loss: 165.33189697265624\n",
      "Training loss: 120.54102783203125\n",
      "Training loss: 164.5600341796875\n",
      "Training loss: 120.0628662109375\n",
      "Training loss: 163.79105224609376\n",
      "Training loss: 119.5863037109375\n",
      "Training loss: 163.025732421875\n",
      "Training loss: 119.11148681640626\n",
      "Training loss: 162.26376953125\n",
      "Training loss: 118.6383056640625\n",
      "Training loss: 161.5051025390625\n",
      "Training loss: 118.1666259765625\n",
      "Training loss: 160.74984130859374\n",
      "Training loss: 117.69632568359376\n",
      "Training loss: 159.99754638671874\n",
      "Training loss: 117.22742919921875\n",
      "Training loss: 159.248486328125\n",
      "Training loss: 116.7606201171875\n",
      "Training loss: 158.503271484375\n",
      "Training loss: 116.29525146484374\n",
      "Training loss: 157.7612548828125\n",
      "Training loss: 115.83162841796874\n",
      "Training loss: 157.0226318359375\n",
      "Training loss: 115.36904296875\n",
      "Training loss: 156.28641357421876\n",
      "Training loss: 114.908251953125\n",
      "Training loss: 155.5539306640625\n",
      "Training loss: 114.44896240234375\n",
      "Training loss: 154.824658203125\n",
      "Training loss: 113.9911865234375\n",
      "Training loss: 154.09876708984376\n",
      "Training loss: 113.53514404296875\n",
      "Training loss: 153.37587890625\n",
      "Training loss: 113.0803955078125\n",
      "Training loss: 152.6560302734375\n",
      "Training loss: 112.6274169921875\n",
      "Training loss: 151.9398193359375\n",
      "Training loss: 112.1759033203125\n",
      "Training loss: 151.226513671875\n",
      "Training loss: 111.7259033203125\n",
      "Training loss: 150.5162353515625\n",
      "Training loss: 111.277197265625\n",
      "Training loss: 149.80904541015624\n",
      "Training loss: 110.83035888671876\n",
      "Training loss: 149.105224609375\n",
      "Training loss: 110.38485107421874\n",
      "Training loss: 148.4044677734375\n",
      "Training loss: 109.94110107421875\n",
      "Training loss: 147.70711669921874\n",
      "Training loss: 109.49888916015625\n",
      "Training loss: 147.01246337890626\n",
      "Training loss: 109.0574951171875\n",
      "Training loss: 146.32027587890624\n",
      "Training loss: 108.6179443359375\n",
      "Training loss: 145.63170166015624\n",
      "Training loss: 108.1798828125\n",
      "Training loss: 144.9459716796875\n",
      "Training loss: 107.7435791015625\n",
      "Training loss: 144.26343994140626\n",
      "Training loss: 107.30843505859374\n",
      "Training loss: 143.58404541015625\n",
      "Training loss: 106.8751220703125\n",
      "Training loss: 142.907666015625\n",
      "Training loss: 106.4433349609375\n",
      "Training loss: 142.23472900390624\n",
      "Training loss: 106.01292724609375\n",
      "Training loss: 141.56429443359374\n",
      "Training loss: 105.5838134765625\n",
      "Training loss: 140.89658203125\n",
      "Training loss: 105.1563720703125\n",
      "Training loss: 140.2323486328125\n",
      "Training loss: 104.73046875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 139.5711669921875\n",
      "Training loss: 104.3061767578125\n",
      "Training loss: 138.9130126953125\n",
      "Training loss: 103.88319091796875\n",
      "Training loss: 138.2573974609375\n",
      "Training loss: 103.46156005859375\n",
      "Training loss: 137.6046142578125\n",
      "Training loss: 103.041552734375\n",
      "Training loss: 136.9550537109375\n",
      "Training loss: 102.62294921875\n",
      "Training loss: 136.30848388671876\n",
      "Training loss: 102.20584106445312\n",
      "Training loss: 135.6647216796875\n",
      "Training loss: 101.79031982421876\n",
      "Training loss: 135.0240478515625\n",
      "Training loss: 101.37643432617188\n",
      "Training loss: 134.38626708984376\n",
      "Training loss: 100.96355590820312\n",
      "Training loss: 133.7510986328125\n",
      "Training loss: 100.55242919921875\n",
      "Training loss: 133.11893310546876\n",
      "Training loss: 100.14251708984375\n",
      "Training loss: 132.489208984375\n",
      "Training loss: 99.73406982421875\n",
      "Training loss: 131.86246337890626\n",
      "Training loss: 99.32705078125\n",
      "Training loss: 131.2387939453125\n",
      "Training loss: 98.92178955078126\n",
      "Training loss: 130.61807861328126\n",
      "Training loss: 98.51776123046875\n",
      "Training loss: 129.99991455078126\n",
      "Training loss: 98.11494140625\n",
      "Training loss: 129.3841552734375\n",
      "Training loss: 97.71370849609374\n",
      "Training loss: 128.77152099609376\n",
      "Training loss: 97.31384887695313\n",
      "Training loss: 128.16148681640624\n",
      "Training loss: 96.91533203125\n",
      "Training loss: 127.5543701171875\n",
      "Training loss: 96.51852416992188\n",
      "Training loss: 126.95009765625\n",
      "Training loss: 96.123193359375\n",
      "Training loss: 126.3487060546875\n",
      "Training loss: 95.72913818359375\n",
      "Training loss: 125.74996337890624\n",
      "Training loss: 95.336376953125\n",
      "Training loss: 125.1534912109375\n",
      "Training loss: 94.9447265625\n",
      "Training loss: 124.5595703125\n",
      "Training loss: 94.55477294921874\n",
      "Training loss: 123.96888427734375\n",
      "Training loss: 94.16651611328125\n",
      "Training loss: 123.38094482421874\n",
      "Training loss: 93.77936401367188\n",
      "Training loss: 122.7954345703125\n",
      "Training loss: 93.39375\n",
      "Training loss: 122.21273193359374\n",
      "Training loss: 93.00943603515626\n",
      "Training loss: 121.63260498046876\n",
      "Training loss: 92.62655029296874\n",
      "Training loss: 121.05513916015624\n",
      "Training loss: 92.24501953125\n",
      "Training loss: 120.48040771484375\n",
      "Training loss: 91.86502075195312\n",
      "Training loss: 119.90828857421874\n",
      "Training loss: 91.48624877929687\n",
      "Training loss: 119.3387451171875\n",
      "Training loss: 91.10899658203125\n",
      "Training loss: 118.771923828125\n",
      "Training loss: 90.73303833007813\n",
      "Training loss: 118.2074462890625\n",
      "Training loss: 90.35811767578124\n",
      "Training loss: 117.64530029296876\n",
      "Training loss: 89.98484497070312\n",
      "Training loss: 117.0860595703125\n",
      "Training loss: 89.61267700195313\n",
      "Training loss: 116.5290283203125\n",
      "Training loss: 89.24210205078126\n",
      "Training loss: 115.97484130859375\n",
      "Training loss: 88.873046875\n",
      "Training loss: 115.4235595703125\n",
      "Training loss: 88.5052734375\n",
      "Training loss: 114.87447509765624\n",
      "Training loss: 88.13865356445312\n",
      "Training loss: 114.32767333984376\n",
      "Training loss: 87.77327270507813\n",
      "Training loss: 113.7832763671875\n",
      "Training loss: 87.409423828125\n",
      "Training loss: 113.2416748046875\n",
      "Training loss: 87.04686279296875\n",
      "Training loss: 112.70257568359375\n",
      "Training loss: 86.68576049804688\n",
      "Training loss: 112.16605224609376\n",
      "Training loss: 86.32611083984375\n",
      "Training loss: 111.63214111328125\n",
      "Training loss: 85.967919921875\n",
      "Training loss: 111.1009521484375\n",
      "Training loss: 85.610791015625\n",
      "Training loss: 110.57156982421876\n",
      "Training loss: 85.25481567382812\n",
      "Training loss: 110.0445068359375\n",
      "Training loss: 84.9001953125\n",
      "Training loss: 109.51998291015624\n",
      "Training loss: 84.54714965820312\n",
      "Training loss: 108.99844970703126\n",
      "Training loss: 84.19525146484375\n",
      "Training loss: 108.4788818359375\n",
      "Training loss: 83.8447265625\n",
      "Training loss: 107.96171875\n",
      "Training loss: 83.49529418945312\n",
      "Training loss: 107.446875\n",
      "Training loss: 83.14744873046875\n",
      "Training loss: 106.9345947265625\n",
      "Training loss: 82.80084228515625\n",
      "Training loss: 106.4246337890625\n",
      "Training loss: 82.45535888671876\n",
      "Training loss: 105.9167724609375\n",
      "Training loss: 82.1111328125\n",
      "Training loss: 105.4114501953125\n",
      "Training loss: 81.76854248046875\n",
      "Training loss: 104.9089111328125\n",
      "Training loss: 81.42733154296874\n",
      "Training loss: 104.40845947265625\n",
      "Training loss: 81.08692626953125\n",
      "Training loss: 103.9098876953125\n",
      "Training loss: 80.7477294921875\n",
      "Training loss: 103.41356201171875\n",
      "Training loss: 80.40991821289063\n",
      "Training loss: 102.9199462890625\n",
      "Training loss: 80.07371215820312\n",
      "Training loss: 102.42867431640624\n",
      "Training loss: 79.73851928710937\n",
      "Training loss: 101.93948974609376\n",
      "Training loss: 79.40467529296875\n",
      "Training loss: 101.452783203125\n",
      "Training loss: 79.07221069335938\n",
      "Training loss: 100.96827392578125\n",
      "Training loss: 78.7408935546875\n",
      "Training loss: 100.48601684570312\n",
      "Training loss: 78.41080322265626\n",
      "Training loss: 100.00613403320312\n",
      "Training loss: 78.08214721679687\n",
      "Training loss: 99.52845458984375\n",
      "Training loss: 77.75452270507813\n",
      "Training loss: 99.05283203125\n",
      "Training loss: 77.42835083007813\n",
      "Training loss: 98.57975463867187\n",
      "Training loss: 77.10335693359374\n",
      "Training loss: 98.10867919921876\n",
      "Training loss: 76.77948608398438\n",
      "Training loss: 97.6396484375\n",
      "Training loss: 76.45682373046876\n",
      "Training loss: 97.17286987304688\n",
      "Training loss: 76.13555297851562\n",
      "Training loss: 96.70857543945313\n",
      "Training loss: 75.81548461914062\n",
      "Training loss: 96.24622802734375\n",
      "Training loss: 75.4965087890625\n",
      "Training loss: 95.78587646484375\n",
      "Training loss: 75.178759765625\n",
      "Training loss: 95.32772216796874\n",
      "Training loss: 74.862353515625\n",
      "Training loss: 94.87185668945312\n",
      "Training loss: 74.54717407226562\n",
      "Training loss: 94.41820068359375\n",
      "Training loss: 74.23284912109375\n",
      "Training loss: 93.96607666015625\n",
      "Training loss: 73.91987915039063\n",
      "Training loss: 93.51646728515625\n",
      "Training loss: 73.60809326171875\n",
      "Training loss: 93.06881103515624\n",
      "Training loss: 73.29749755859375\n",
      "Training loss: 92.62337036132813\n",
      "Training loss: 72.98834228515625\n",
      "Training loss: 92.18010864257812\n",
      "Training loss: 72.68003540039062\n",
      "Training loss: 91.73854370117188\n",
      "Training loss: 72.37291259765625\n",
      "Training loss: 91.29909057617188\n",
      "Training loss: 72.06721801757813\n",
      "Training loss: 90.86214599609374\n",
      "Training loss: 71.76263427734375\n",
      "Training loss: 90.42693481445312\n",
      "Training loss: 71.45899047851563\n",
      "Training loss: 89.99356689453126\n",
      "Training loss: 71.15667114257812\n",
      "Training loss: 89.562548828125\n",
      "Training loss: 70.85557861328125\n",
      "Training loss: 89.133544921875\n",
      "Training loss: 70.55569458007812\n",
      "Training loss: 88.70648193359375\n",
      "Training loss: 70.2568603515625\n",
      "Training loss: 88.28136596679687\n",
      "Training loss: 69.95924072265625\n",
      "Training loss: 87.85841674804688\n",
      "Training loss: 69.66292114257813\n",
      "Training loss: 87.43756103515625\n",
      "Training loss: 69.3675537109375\n",
      "Training loss: 87.0185546875\n",
      "Training loss: 69.07376708984376\n",
      "Training loss: 86.60185546875\n",
      "Training loss: 68.78099975585937\n",
      "Training loss: 86.18707885742188\n",
      "Training loss: 68.48896484375\n",
      "Training loss: 85.77355346679687\n",
      "Training loss: 68.19810791015625\n",
      "Training loss: 85.36232299804688\n",
      "Training loss: 67.90859985351562\n",
      "Training loss: 84.95304565429687\n",
      "Training loss: 67.62010498046875\n",
      "Training loss: 84.54581909179687\n",
      "Training loss: 67.33292236328126\n",
      "Training loss: 84.14051513671875\n",
      "Training loss: 67.04669799804688\n",
      "Training loss: 83.737158203125\n",
      "Training loss: 66.76181640625\n",
      "Training loss: 83.33582153320313\n",
      "Training loss: 66.47802734375\n",
      "Training loss: 82.93619384765626\n",
      "Training loss: 66.19521484375\n",
      "Training loss: 82.53861083984376\n",
      "Training loss: 65.91363525390625\n",
      "Training loss: 82.14281005859375\n",
      "Training loss: 65.633056640625\n",
      "Training loss: 81.74885864257813\n",
      "Training loss: 65.35369873046875\n",
      "Training loss: 81.35691528320312\n",
      "Training loss: 65.07545776367188\n",
      "Training loss: 80.96686401367188\n",
      "Training loss: 64.79833374023437\n",
      "Training loss: 80.57865600585937\n",
      "Training loss: 64.52208251953125\n",
      "Training loss: 80.19188842773437\n",
      "Training loss: 64.24684448242188\n",
      "Training loss: 79.80687866210937\n",
      "Training loss: 63.97269897460937\n",
      "Training loss: 79.42391357421874\n",
      "Training loss: 63.6998046875\n",
      "Training loss: 79.04290771484375\n",
      "Training loss: 63.427972412109376\n",
      "Training loss: 78.66353759765624\n",
      "Training loss: 63.157403564453126\n",
      "Training loss: 78.28642578125\n",
      "Training loss: 62.88790283203125\n",
      "Training loss: 77.91065673828125\n",
      "Training loss: 62.61903686523438\n",
      "Training loss: 77.53656616210938\n",
      "Training loss: 62.35142211914062\n",
      "Training loss: 77.16436767578125\n",
      "Training loss: 62.08504028320313\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 76.79415283203124\n",
      "Training loss: 61.81959228515625\n",
      "Training loss: 76.42551879882812\n",
      "Training loss: 61.555419921875\n",
      "Training loss: 76.05894775390625\n",
      "Training loss: 61.292083740234375\n",
      "Training loss: 75.6937255859375\n",
      "Training loss: 61.02989501953125\n",
      "Training loss: 75.330517578125\n",
      "Training loss: 60.76864013671875\n",
      "Training loss: 74.96881103515625\n",
      "Training loss: 60.508660888671876\n",
      "Training loss: 74.60922241210938\n",
      "Training loss: 60.24957275390625\n",
      "Training loss: 74.25087890625\n",
      "Training loss: 59.991455078125\n",
      "Training loss: 73.89442138671875\n",
      "Training loss: 59.73443603515625\n",
      "Training loss: 73.53972778320312\n",
      "Training loss: 59.47847900390625\n",
      "Training loss: 73.18670654296875\n",
      "Training loss: 59.22357177734375\n",
      "Training loss: 72.83535766601562\n",
      "Training loss: 58.96949462890625\n",
      "Training loss: 72.48543701171874\n",
      "Training loss: 58.716339111328125\n",
      "Training loss: 72.13710327148438\n",
      "Training loss: 58.4642822265625\n",
      "Training loss: 71.7906005859375\n",
      "Training loss: 58.213507080078124\n",
      "Training loss: 71.44609985351562\n",
      "Training loss: 57.963812255859374\n",
      "Training loss: 71.1033203125\n",
      "Training loss: 57.71500854492187\n",
      "Training loss: 70.76176147460937\n",
      "Training loss: 57.46707763671875\n",
      "Training loss: 70.42213745117188\n",
      "Training loss: 57.22035522460938\n",
      "Training loss: 70.084033203125\n",
      "Training loss: 56.97440185546875\n",
      "Training loss: 69.74742431640625\n",
      "Training loss: 56.72972412109375\n",
      "Training loss: 69.41270751953125\n",
      "Training loss: 56.485833740234376\n",
      "Training loss: 69.07939453125\n",
      "Training loss: 56.24299926757813\n",
      "Training loss: 68.74768676757813\n",
      "Training loss: 56.001129150390625\n",
      "Training loss: 68.41768798828124\n",
      "Training loss: 55.760321044921874\n",
      "Training loss: 68.0891845703125\n",
      "Training loss: 55.520379638671876\n",
      "Training loss: 67.76221313476563\n",
      "Training loss: 55.28132934570313\n",
      "Training loss: 67.43674926757812\n",
      "Training loss: 55.0434326171875\n",
      "Training loss: 67.11306762695312\n",
      "Training loss: 54.80653686523438\n",
      "Training loss: 66.7908935546875\n",
      "Training loss: 54.57042236328125\n",
      "Training loss: 66.470068359375\n",
      "Training loss: 54.33528442382813\n",
      "Training loss: 66.15079345703126\n",
      "Training loss: 54.10099487304687\n",
      "Training loss: 65.83285522460938\n",
      "Training loss: 53.86771240234375\n",
      "Training loss: 65.51668090820313\n",
      "Training loss: 53.63555908203125\n",
      "Training loss: 65.20209350585938\n",
      "Training loss: 53.40427856445312\n",
      "Training loss: 64.88909912109375\n",
      "Training loss: 53.17408447265625\n",
      "Training loss: 64.57764282226563\n",
      "Training loss: 52.944671630859375\n",
      "Training loss: 64.26756591796875\n",
      "Training loss: 52.716241455078126\n",
      "Training loss: 63.95899658203125\n",
      "Training loss: 52.4888671875\n",
      "Training loss: 63.65196533203125\n",
      "Training loss: 52.26229248046875\n",
      "Training loss: 63.34637451171875\n",
      "Training loss: 52.03656005859375\n",
      "Training loss: 63.042047119140626\n",
      "Training loss: 51.8119140625\n",
      "Training loss: 62.7394775390625\n",
      "Training loss: 51.5883056640625\n",
      "Training loss: 62.438470458984376\n",
      "Training loss: 51.36549682617188\n",
      "Training loss: 62.13875732421875\n",
      "Training loss: 51.14345092773438\n",
      "Training loss: 61.84033203125\n",
      "Training loss: 50.92250061035156\n",
      "Training loss: 61.54334716796875\n",
      "Training loss: 50.70228576660156\n",
      "Training loss: 61.24800415039063\n",
      "Training loss: 50.483148193359376\n",
      "Training loss: 60.95402221679687\n",
      "Training loss: 50.2649169921875\n",
      "Training loss: 60.661431884765626\n",
      "Training loss: 50.04732971191406\n",
      "Training loss: 60.370001220703124\n",
      "Training loss: 49.830865478515626\n",
      "Training loss: 60.08018798828125\n",
      "Training loss: 49.615194702148436\n",
      "Training loss: 59.79180908203125\n",
      "Training loss: 49.40061645507812\n",
      "Training loss: 59.50484008789063\n",
      "Training loss: 49.18656616210937\n",
      "Training loss: 59.219140625\n",
      "Training loss: 48.973690795898435\n",
      "Training loss: 58.93487548828125\n",
      "Training loss: 48.76155395507813\n",
      "Training loss: 58.652001953125\n",
      "Training loss: 48.55043334960938\n",
      "Training loss: 58.37046508789062\n",
      "Training loss: 48.340118408203125\n",
      "Training loss: 58.09039306640625\n",
      "Training loss: 48.130917358398435\n",
      "Training loss: 57.81196899414063\n",
      "Training loss: 47.92252807617187\n",
      "Training loss: 57.53472900390625\n",
      "Training loss: 47.714794921875\n",
      "Training loss: 57.25858154296875\n",
      "Training loss: 47.50803833007812\n",
      "Training loss: 56.98396606445313\n",
      "Training loss: 47.302182006835935\n",
      "Training loss: 56.71072998046875\n",
      "Training loss: 47.097137451171875\n",
      "Training loss: 56.438604736328124\n",
      "Training loss: 46.89274291992187\n",
      "Training loss: 56.16761474609375\n",
      "Training loss: 46.689413452148436\n",
      "Training loss: 55.89827270507813\n",
      "Training loss: 46.48678283691406\n",
      "Training loss: 55.63004150390625\n",
      "Training loss: 46.2852294921875\n",
      "Training loss: 55.36337890625\n",
      "Training loss: 46.08442993164063\n",
      "Training loss: 55.09785766601563\n",
      "Training loss: 45.88447875976563\n",
      "Training loss: 54.83358154296875\n",
      "Training loss: 45.685366821289065\n",
      "Training loss: 54.57078857421875\n",
      "Training loss: 45.48706970214844\n",
      "Training loss: 54.309136962890626\n",
      "Training loss: 45.28969116210938\n",
      "Training loss: 54.04891357421875\n",
      "Training loss: 45.093072509765626\n",
      "Training loss: 53.78970947265625\n",
      "Training loss: 44.89708557128906\n",
      "Training loss: 53.5317138671875\n",
      "Training loss: 44.702099609375\n",
      "Training loss: 53.27510986328125\n",
      "Training loss: 44.50791015625\n",
      "Training loss: 53.01973876953125\n",
      "Training loss: 44.314517211914065\n",
      "Training loss: 52.76549072265625\n",
      "Training loss: 44.121884155273435\n",
      "Training loss: 52.51265258789063\n",
      "Training loss: 43.93033142089844\n",
      "Training loss: 52.2611083984375\n",
      "Training loss: 43.739453125\n",
      "Training loss: 52.0106689453125\n",
      "Training loss: 43.549267578125\n",
      "Training loss: 51.761419677734374\n",
      "Training loss: 43.36000366210938\n",
      "Training loss: 51.513519287109375\n",
      "Training loss: 43.17142944335937\n",
      "Training loss: 51.26670532226562\n",
      "Training loss: 42.983822631835935\n",
      "Training loss: 51.02128295898437\n",
      "Training loss: 42.796914672851564\n",
      "Training loss: 50.776800537109374\n",
      "Training loss: 42.61067504882813\n",
      "Training loss: 50.5336181640625\n",
      "Training loss: 42.42545776367187\n",
      "Training loss: 50.2916748046875\n",
      "Training loss: 42.24091796875\n",
      "Training loss: 50.050714111328126\n",
      "Training loss: 42.05689392089844\n",
      "Training loss: 49.810919189453124\n",
      "Training loss: 41.873956298828126\n",
      "Training loss: 49.572470092773436\n",
      "Training loss: 41.69178161621094\n",
      "Training loss: 49.335147094726565\n",
      "Training loss: 41.51027221679688\n",
      "Training loss: 49.098959350585936\n",
      "Training loss: 41.3296142578125\n",
      "Training loss: 48.863983154296875\n",
      "Training loss: 41.14977111816406\n",
      "Training loss: 48.63025817871094\n",
      "Training loss: 40.970767211914065\n",
      "Training loss: 48.39760131835938\n",
      "Training loss: 40.792303466796874\n",
      "Training loss: 48.16597290039063\n",
      "Training loss: 40.61472778320312\n",
      "Training loss: 47.935498046875\n",
      "Training loss: 40.43781433105469\n",
      "Training loss: 47.7060791015625\n",
      "Training loss: 40.261688232421875\n",
      "Training loss: 47.477944946289064\n",
      "Training loss: 40.086419677734376\n",
      "Training loss: 47.250912475585935\n",
      "Training loss: 39.911834716796875\n",
      "Training loss: 47.024862670898436\n",
      "Training loss: 39.737796020507815\n",
      "Training loss: 46.79990234375\n",
      "Training loss: 39.56474304199219\n",
      "Training loss: 46.57625122070313\n",
      "Training loss: 39.39250793457031\n",
      "Training loss: 46.3536376953125\n",
      "Training loss: 39.22071838378906\n",
      "Training loss: 46.131893920898435\n",
      "Training loss: 39.04984741210937\n",
      "Training loss: 45.91135864257812\n",
      "Training loss: 38.87957763671875\n",
      "Training loss: 45.69190979003906\n",
      "Training loss: 38.71018981933594\n",
      "Training loss: 45.47371215820313\n",
      "Training loss: 38.541543579101564\n",
      "Training loss: 45.25650329589844\n",
      "Training loss: 38.37364196777344\n",
      "Training loss: 45.04033203125\n",
      "Training loss: 38.206353759765626\n",
      "Training loss: 44.8251953125\n",
      "Training loss: 38.039862060546874\n",
      "Training loss: 44.611077880859376\n",
      "Training loss: 37.87391052246094\n",
      "Training loss: 44.39795837402344\n",
      "Training loss: 37.708798217773435\n",
      "Training loss: 44.185968017578126\n",
      "Training loss: 37.54439697265625\n",
      "Training loss: 43.9750244140625\n",
      "Training loss: 37.380682373046874\n",
      "Training loss: 43.765106201171875\n",
      "Training loss: 37.21781616210937\n",
      "Training loss: 43.556356811523436\n",
      "Training loss: 37.055459594726564\n",
      "Training loss: 43.348333740234374\n",
      "Training loss: 36.893829345703125\n",
      "Training loss: 43.14143676757813\n",
      "Training loss: 36.73287353515625\n",
      "Training loss: 42.93553771972656\n",
      "Training loss: 36.572653198242186\n",
      "Training loss: 42.73072814941406\n",
      "Training loss: 36.41301574707031\n",
      "Training loss: 42.526727294921876\n",
      "Training loss: 36.254180908203125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 42.32389526367187\n",
      "Training loss: 36.096038818359375\n",
      "Training loss: 42.12207946777344\n",
      "Training loss: 35.93851013183594\n",
      "Training loss: 41.921148681640624\n",
      "Training loss: 35.78172912597656\n",
      "Training loss: 41.721282958984375\n",
      "Training loss: 35.62568969726563\n",
      "Training loss: 41.52249450683594\n",
      "Training loss: 35.470245361328125\n",
      "Training loss: 41.324658203125\n",
      "Training loss: 35.31559753417969\n",
      "Training loss: 41.12777404785156\n",
      "Training loss: 35.16141967773437\n",
      "Training loss: 40.931765747070315\n",
      "Training loss: 35.008123779296874\n",
      "Training loss: 40.73679809570312\n",
      "Training loss: 34.855282592773435\n",
      "Training loss: 40.54274291992188\n",
      "Training loss: 34.70320434570313\n",
      "Training loss: 40.34961853027344\n",
      "Training loss: 34.551797485351564\n",
      "Training loss: 40.15756530761719\n",
      "Training loss: 34.401101684570314\n",
      "Training loss: 39.966452026367186\n",
      "Training loss: 34.250970458984376\n",
      "Training loss: 39.77626342773438\n",
      "Training loss: 34.10160522460937\n",
      "Training loss: 39.58699035644531\n",
      "Training loss: 33.952737426757814\n",
      "Training loss: 39.398492431640626\n",
      "Training loss: 33.80463562011719\n",
      "Training loss: 39.211151123046875\n",
      "Training loss: 33.65712280273438\n",
      "Training loss: 39.02463989257812\n",
      "Training loss: 33.51011352539062\n",
      "Training loss: 38.838766479492186\n",
      "Training loss: 33.36378784179688\n",
      "Training loss: 38.654046630859376\n",
      "Training loss: 33.21812744140625\n",
      "Training loss: 38.47020568847656\n",
      "Training loss: 33.07325439453125\n",
      "Training loss: 38.287420654296874\n",
      "Training loss: 32.92892761230469\n",
      "Training loss: 38.1054443359375\n",
      "Training loss: 32.785299682617186\n",
      "Training loss: 37.924468994140625\n",
      "Training loss: 32.642214965820315\n",
      "Training loss: 37.744265747070315\n",
      "Training loss: 32.49979858398437\n",
      "Training loss: 37.56501159667969\n",
      "Training loss: 32.35792846679688\n",
      "Training loss: 37.38646240234375\n",
      "Training loss: 32.21681213378906\n",
      "Training loss: 37.20904541015625\n",
      "Training loss: 32.076211547851564\n",
      "Training loss: 37.032354736328124\n",
      "Training loss: 31.93616638183594\n",
      "Training loss: 36.85653381347656\n",
      "Training loss: 31.796795654296876\n",
      "Training loss: 36.68157958984375\n",
      "Training loss: 31.658123779296876\n",
      "Training loss: 36.507513427734374\n",
      "Training loss: 31.519918823242186\n",
      "Training loss: 36.334371948242186\n",
      "Training loss: 31.3823974609375\n",
      "Training loss: 36.16195068359375\n",
      "Training loss: 31.24529113769531\n",
      "Training loss: 35.990264892578125\n",
      "Training loss: 31.1089599609375\n",
      "Training loss: 35.819610595703125\n",
      "Training loss: 30.973248291015626\n",
      "Training loss: 35.64985961914063\n",
      "Training loss: 30.838101196289063\n",
      "Training loss: 35.480691528320314\n",
      "Training loss: 30.703436279296874\n",
      "Training loss: 35.312548828125\n",
      "Training loss: 30.569525146484374\n",
      "Training loss: 35.145233154296875\n",
      "Training loss: 30.43614807128906\n",
      "Training loss: 34.978701782226565\n",
      "Training loss: 30.30338134765625\n",
      "Training loss: 34.813009643554686\n",
      "Training loss: 30.171096801757812\n",
      "Training loss: 34.648129272460935\n",
      "Training loss: 30.03941955566406\n",
      "Training loss: 34.48394775390625\n",
      "Training loss: 29.908404541015624\n",
      "Training loss: 34.3207275390625\n",
      "Training loss: 29.777838134765624\n",
      "Training loss: 34.158206176757815\n",
      "Training loss: 29.647885131835938\n",
      "Training loss: 33.996612548828125\n",
      "Training loss: 29.51859130859375\n",
      "Training loss: 33.835662841796875\n",
      "Training loss: 29.389706420898438\n",
      "Training loss: 33.675473022460935\n",
      "Training loss: 29.26148376464844\n",
      "Training loss: 33.51620178222656\n",
      "Training loss: 29.133786010742188\n",
      "Training loss: 33.357598876953126\n",
      "Training loss: 29.00676574707031\n",
      "Training loss: 33.200079345703124\n",
      "Training loss: 28.880364990234376\n",
      "Training loss: 33.043218994140624\n",
      "Training loss: 28.75457763671875\n",
      "Training loss: 32.88731079101562\n",
      "Training loss: 28.629095458984374\n",
      "Training loss: 32.73177490234375\n",
      "Training loss: 28.504299926757813\n",
      "Training loss: 32.57718505859375\n",
      "Training loss: 28.37995300292969\n",
      "Training loss: 32.423388671875\n",
      "Training loss: 28.256365966796874\n",
      "Training loss: 32.270477294921875\n",
      "Training loss: 28.13328857421875\n",
      "Training loss: 32.11817321777344\n",
      "Training loss: 28.010614013671876\n",
      "Training loss: 31.9666748046875\n",
      "Training loss: 27.888583374023437\n",
      "Training loss: 31.815936279296874\n",
      "Training loss: 27.767117309570313\n",
      "Training loss: 31.665924072265625\n",
      "Training loss: 27.64617004394531\n",
      "Training loss: 31.5167236328125\n",
      "Training loss: 27.525811767578126\n",
      "Training loss: 31.368313598632813\n",
      "Training loss: 27.405950927734374\n",
      "Training loss: 31.220443725585938\n",
      "Training loss: 27.286541748046876\n",
      "Training loss: 31.073345947265626\n",
      "Training loss: 27.167706298828126\n",
      "Training loss: 30.927069091796874\n",
      "Training loss: 27.049456787109374\n",
      "Training loss: 30.78144226074219\n",
      "Training loss: 26.93152770996094\n",
      "Training loss: 30.636447143554687\n",
      "Training loss: 26.814242553710937\n",
      "Training loss: 30.492236328125\n",
      "Training loss: 26.697531127929686\n",
      "Training loss: 30.34880676269531\n",
      "Training loss: 26.581399536132814\n",
      "Training loss: 30.2061279296875\n",
      "Training loss: 26.465740966796876\n",
      "Training loss: 30.064126586914064\n",
      "Training loss: 26.350567626953126\n",
      "Training loss: 29.922756958007813\n",
      "Training loss: 26.235760498046876\n",
      "Training loss: 29.7820068359375\n",
      "Training loss: 26.121612548828125\n",
      "Training loss: 29.642034912109374\n",
      "Training loss: 26.007940673828124\n",
      "Training loss: 29.502716064453125\n",
      "Training loss: 25.894735717773436\n",
      "Training loss: 29.364120483398438\n",
      "Training loss: 25.78204345703125\n",
      "Training loss: 29.226239013671876\n",
      "Training loss: 25.669903564453126\n",
      "Training loss: 29.088983154296876\n",
      "Training loss: 25.558145141601564\n",
      "Training loss: 28.95225830078125\n",
      "Training loss: 25.446923828125\n",
      "Training loss: 28.816412353515624\n",
      "Training loss: 25.33624725341797\n",
      "Training loss: 28.68121337890625\n",
      "Training loss: 25.226123046875\n",
      "Training loss: 28.546746826171876\n",
      "Training loss: 25.11646423339844\n",
      "Training loss: 28.412921142578124\n",
      "Training loss: 25.007308959960938\n",
      "Training loss: 28.279681396484374\n",
      "Training loss: 24.898513793945312\n",
      "Training loss: 28.147116088867186\n",
      "Training loss: 24.79027404785156\n",
      "Training loss: 28.01519775390625\n",
      "Training loss: 24.682614135742188\n",
      "Training loss: 27.884124755859375\n",
      "Training loss: 24.57552490234375\n",
      "Training loss: 27.753778076171876\n",
      "Training loss: 24.468936157226562\n",
      "Training loss: 27.624005126953126\n",
      "Training loss: 24.3626953125\n",
      "Training loss: 27.494833374023436\n",
      "Training loss: 24.25703125\n",
      "Training loss: 27.366305541992187\n",
      "Training loss: 24.151704406738283\n",
      "Training loss: 27.238345336914062\n",
      "Training loss: 24.046781921386717\n",
      "Training loss: 27.110888671875\n",
      "Training loss: 23.94241943359375\n",
      "Training loss: 26.98426513671875\n",
      "Training loss: 23.83856658935547\n",
      "Training loss: 26.85823974609375\n",
      "Training loss: 23.735153198242188\n",
      "Training loss: 26.732757568359375\n",
      "Training loss: 23.63213348388672\n",
      "Training loss: 26.6078369140625\n",
      "Training loss: 23.52971496582031\n",
      "Training loss: 26.483804321289064\n",
      "Training loss: 23.427731323242188\n",
      "Training loss: 26.360284423828126\n",
      "Training loss: 23.326113891601562\n",
      "Training loss: 26.237307739257812\n",
      "Training loss: 23.224995422363282\n",
      "Training loss: 26.114950561523436\n",
      "Training loss: 23.1243408203125\n",
      "Training loss: 25.993240356445312\n",
      "Training loss: 23.02427520751953\n",
      "Training loss: 25.872232055664064\n",
      "Training loss: 22.92449951171875\n",
      "Training loss: 25.75166015625\n",
      "Training loss: 22.825228881835937\n",
      "Training loss: 25.631881713867188\n",
      "Training loss: 22.72642364501953\n",
      "Training loss: 25.512454223632812\n",
      "Training loss: 22.62792205810547\n",
      "Training loss: 25.39354705810547\n",
      "Training loss: 22.52986297607422\n",
      "Training loss: 25.27544250488281\n",
      "Training loss: 22.432369995117188\n",
      "Training loss: 25.15790252685547\n",
      "Training loss: 22.335295104980467\n",
      "Training loss: 25.040890502929688\n",
      "Training loss: 22.2385498046875\n",
      "Training loss: 24.924398803710936\n",
      "Training loss: 22.142398071289062\n",
      "Training loss: 24.808622741699217\n",
      "Training loss: 22.04651336669922\n",
      "Training loss: 24.693301391601562\n",
      "Training loss: 21.951132202148436\n",
      "Training loss: 24.578536987304688\n",
      "Training loss: 21.856228637695313\n",
      "Training loss: 24.464480590820312\n",
      "Training loss: 21.761738586425782\n",
      "Training loss: 24.350949096679688\n",
      "Training loss: 21.667703247070314\n",
      "Training loss: 24.23798065185547\n",
      "Training loss: 21.574076843261718\n",
      "Training loss: 24.12561798095703\n",
      "Training loss: 21.48091278076172\n",
      "Training loss: 24.01380615234375\n",
      "Training loss: 21.388102722167968\n",
      "Training loss: 23.902487182617186\n",
      "Training loss: 21.295822143554688\n",
      "Training loss: 23.79185791015625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 21.203897094726564\n",
      "Training loss: 23.681620788574218\n",
      "Training loss: 21.112277221679687\n",
      "Training loss: 23.571835327148438\n",
      "Training loss: 21.02109375\n",
      "Training loss: 23.46277618408203\n",
      "Training loss: 20.930538940429688\n",
      "Training loss: 23.35437774658203\n",
      "Training loss: 20.84021453857422\n",
      "Training loss: 23.24635009765625\n",
      "Training loss: 20.75036163330078\n",
      "Training loss: 23.138825988769533\n",
      "Training loss: 20.66084442138672\n",
      "Training loss: 23.03193054199219\n",
      "Training loss: 20.57189178466797\n",
      "Training loss: 22.925578308105468\n",
      "Training loss: 20.48321075439453\n",
      "Training loss: 22.81968536376953\n",
      "Training loss: 20.394882202148438\n",
      "Training loss: 22.71424255371094\n",
      "Training loss: 20.307005310058592\n",
      "Training loss: 22.60943603515625\n",
      "Training loss: 20.219515991210937\n",
      "Training loss: 22.505099487304687\n",
      "Training loss: 20.132443237304688\n",
      "Training loss: 22.401248168945312\n",
      "Training loss: 20.045718383789062\n",
      "Training loss: 22.298002624511717\n",
      "Training loss: 19.95953826904297\n",
      "Training loss: 22.19536437988281\n",
      "Training loss: 19.873684692382813\n",
      "Training loss: 22.093063354492188\n",
      "Training loss: 19.788186645507814\n",
      "Training loss: 21.991300964355467\n",
      "Training loss: 19.702978515625\n",
      "Training loss: 21.890054321289064\n",
      "Training loss: 19.618307495117186\n",
      "Training loss: 21.789274597167967\n",
      "Training loss: 19.53394470214844\n",
      "Training loss: 21.688984680175782\n",
      "Training loss: 19.449859619140625\n",
      "Training loss: 21.589212036132814\n",
      "Training loss: 19.36634979248047\n",
      "Training loss: 21.48997344970703\n",
      "Training loss: 19.283123779296876\n",
      "Training loss: 21.39119873046875\n",
      "Training loss: 19.200234985351564\n",
      "Training loss: 21.29289093017578\n",
      "Training loss: 19.11781463623047\n",
      "Training loss: 21.19512939453125\n",
      "Training loss: 19.035745239257814\n",
      "Training loss: 21.097775268554688\n",
      "Training loss: 18.954020690917968\n",
      "Training loss: 21.000895690917968\n",
      "Training loss: 18.872607421875\n",
      "Training loss: 20.904560852050782\n",
      "Training loss: 18.79167785644531\n",
      "Training loss: 20.808688354492187\n",
      "Training loss: 18.71104278564453\n",
      "Training loss: 20.713276672363282\n",
      "Training loss: 18.63082275390625\n",
      "Training loss: 20.618408203125\n",
      "Training loss: 18.550926208496094\n",
      "Training loss: 20.52391357421875\n",
      "Training loss: 18.47148895263672\n",
      "Training loss: 20.43004608154297\n",
      "Training loss: 18.392364501953125\n",
      "Training loss: 20.336546325683592\n",
      "Training loss: 18.313633728027344\n",
      "Training loss: 20.24351806640625\n",
      "Training loss: 18.235263061523437\n",
      "Training loss: 20.151051330566407\n",
      "Training loss: 18.15730438232422\n",
      "Training loss: 20.05901641845703\n",
      "Training loss: 18.079621887207033\n",
      "Training loss: 19.967376708984375\n",
      "Training loss: 18.002239990234376\n",
      "Training loss: 19.876153564453126\n",
      "Training loss: 17.925273132324218\n",
      "Training loss: 19.785400390625\n",
      "Training loss: 17.848617553710938\n",
      "Training loss: 19.695088195800782\n",
      "Training loss: 17.772323608398438\n",
      "Training loss: 19.60531005859375\n",
      "Training loss: 17.696441650390625\n",
      "Training loss: 19.515931701660158\n",
      "Training loss: 17.620870971679686\n",
      "Training loss: 19.426980590820314\n",
      "Training loss: 17.545611572265624\n",
      "Training loss: 19.338525390625\n",
      "Training loss: 17.4707763671875\n",
      "Training loss: 19.250503540039062\n",
      "Training loss: 17.396263122558594\n",
      "Training loss: 19.1629150390625\n",
      "Training loss: 17.32203063964844\n",
      "Training loss: 19.07579650878906\n",
      "Training loss: 17.24835205078125\n",
      "Training loss: 18.9891845703125\n",
      "Training loss: 17.174879455566405\n",
      "Training loss: 18.90294494628906\n",
      "Training loss: 17.101687622070312\n",
      "Training loss: 18.817048645019533\n",
      "Training loss: 17.028828430175782\n",
      "Training loss: 18.73161163330078\n",
      "Training loss: 16.95641326904297\n",
      "Training loss: 18.646701049804687\n",
      "Training loss: 16.88428192138672\n",
      "Training loss: 18.562176513671876\n",
      "Training loss: 16.81243896484375\n",
      "Training loss: 18.477992248535156\n",
      "Training loss: 16.741000366210937\n",
      "Training loss: 18.394284057617188\n",
      "Training loss: 16.669760131835936\n",
      "Training loss: 18.310891723632814\n",
      "Training loss: 16.598908996582033\n",
      "Training loss: 18.22801055908203\n",
      "Training loss: 16.528361511230468\n",
      "Training loss: 18.145425415039064\n",
      "Training loss: 16.458062744140626\n",
      "Training loss: 18.06328430175781\n",
      "Training loss: 16.38813018798828\n",
      "Training loss: 17.98155975341797\n",
      "Training loss: 16.318569946289063\n",
      "Training loss: 17.900326538085938\n",
      "Training loss: 16.24950256347656\n",
      "Training loss: 17.81952667236328\n",
      "Training loss: 16.180416870117188\n",
      "Training loss: 17.73893585205078\n",
      "Training loss: 16.111715698242186\n",
      "Training loss: 17.658706665039062\n",
      "Training loss: 16.043374633789064\n",
      "Training loss: 17.579014587402344\n",
      "Training loss: 15.975404357910156\n",
      "Training loss: 17.499734497070314\n",
      "Training loss: 15.907723999023437\n",
      "Training loss: 17.42083740234375\n",
      "Training loss: 15.8403076171875\n",
      "Training loss: 17.34235076904297\n",
      "Training loss: 15.773324584960937\n",
      "Training loss: 17.264315795898437\n",
      "Training loss: 15.70659942626953\n",
      "Training loss: 17.186590576171874\n",
      "Training loss: 15.640208435058593\n",
      "Training loss: 17.10936584472656\n",
      "Training loss: 15.574136352539062\n",
      "Training loss: 17.032464599609376\n",
      "Training loss: 15.5084228515625\n",
      "Training loss: 16.956027221679687\n",
      "Training loss: 15.442889404296874\n",
      "Training loss: 16.879788208007813\n",
      "Training loss: 15.377700805664062\n",
      "Training loss: 16.804095458984374\n",
      "Training loss: 15.312857055664063\n",
      "Training loss: 16.728781127929686\n",
      "Training loss: 15.248338317871093\n",
      "Training loss: 16.653794860839845\n",
      "Training loss: 15.18406982421875\n",
      "Training loss: 16.579229736328124\n",
      "Training loss: 15.120098876953126\n",
      "Training loss: 16.50501403808594\n",
      "Training loss: 15.056498718261718\n",
      "Training loss: 16.43116149902344\n",
      "Training loss: 14.993074035644531\n",
      "Training loss: 16.35765686035156\n",
      "Training loss: 14.929960632324219\n",
      "Training loss: 16.284452819824217\n",
      "Training loss: 14.86707000732422\n",
      "Training loss: 16.211524963378906\n",
      "Training loss: 14.804513549804687\n",
      "Training loss: 16.139179992675782\n",
      "Training loss: 14.74239501953125\n",
      "Training loss: 16.0672119140625\n",
      "Training loss: 14.680477905273438\n",
      "Training loss: 15.99560089111328\n",
      "Training loss: 14.618858337402344\n",
      "Training loss: 15.9241943359375\n",
      "Training loss: 14.557376098632812\n",
      "Training loss: 15.8531005859375\n",
      "Training loss: 14.496292114257812\n",
      "Training loss: 15.782510375976562\n",
      "Training loss: 14.435501098632812\n",
      "Training loss: 15.712222290039062\n",
      "Training loss: 14.375021362304688\n",
      "Training loss: 15.642366027832031\n",
      "Training loss: 14.314852905273437\n",
      "Training loss: 15.572860717773438\n",
      "Training loss: 14.254911804199219\n",
      "Training loss: 15.50360565185547\n",
      "Training loss: 14.195193481445312\n",
      "Training loss: 15.434745788574219\n",
      "Training loss: 14.135765075683594\n",
      "Training loss: 15.366159057617187\n",
      "Training loss: 14.076644897460938\n",
      "Training loss: 15.298017883300782\n",
      "Training loss: 14.017776489257812\n",
      "Training loss: 15.230128479003906\n",
      "Training loss: 13.959202575683594\n",
      "Training loss: 15.162551879882812\n",
      "Training loss: 13.900825500488281\n",
      "Training loss: 15.095303344726563\n",
      "Training loss: 13.842770385742188\n",
      "Training loss: 15.028482055664062\n",
      "Training loss: 13.785026550292969\n",
      "Training loss: 14.96197509765625\n",
      "Training loss: 13.727493286132812\n",
      "Training loss: 14.895777893066406\n",
      "Training loss: 13.670268249511718\n",
      "Training loss: 14.829910278320312\n",
      "Training loss: 13.613314819335937\n",
      "Training loss: 14.764425659179688\n",
      "Training loss: 13.556578063964844\n",
      "Training loss: 14.699209594726563\n",
      "Training loss: 13.500119018554688\n",
      "Training loss: 14.634335327148438\n",
      "Training loss: 13.443968200683594\n",
      "Training loss: 14.569789123535156\n",
      "Training loss: 13.388076782226562\n",
      "Training loss: 14.505575561523438\n",
      "Training loss: 13.33239288330078\n",
      "Training loss: 14.441688537597656\n",
      "Training loss: 13.276951599121094\n",
      "Training loss: 14.3780517578125\n",
      "Training loss: 13.221833801269531\n",
      "Training loss: 14.314830017089843\n",
      "Training loss: 13.167068481445312\n",
      "Training loss: 14.251994323730468\n",
      "Training loss: 13.112385559082032\n",
      "Training loss: 14.189254760742188\n",
      "Training loss: 13.057951354980469\n",
      "Training loss: 14.126895141601562\n",
      "Training loss: 13.00379638671875\n",
      "Training loss: 14.064912414550781\n",
      "Training loss: 12.949986267089844\n",
      "Training loss: 14.003279113769532\n",
      "Training loss: 12.896424865722656\n",
      "Training loss: 13.941909790039062\n",
      "Training loss: 12.843087768554687\n",
      "Training loss: 13.880810546875\n",
      "Training loss: 12.789870452880859\n",
      "Training loss: 13.820027160644532\n",
      "Training loss: 12.737086486816406\n",
      "Training loss: 13.759613037109375\n",
      "Training loss: 12.684492492675782\n",
      "Training loss: 13.69940643310547\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 12.632078552246094\n",
      "Training loss: 13.639541625976562\n",
      "Training loss: 12.579924011230469\n",
      "Training loss: 13.579937744140626\n",
      "Training loss: 12.528025054931641\n",
      "Training loss: 13.520700073242187\n",
      "Training loss: 12.476461029052734\n",
      "Training loss: 13.46178741455078\n",
      "Training loss: 12.425099182128907\n",
      "Training loss: 13.403178405761718\n",
      "Training loss: 12.374015045166015\n",
      "Training loss: 13.344828796386718\n",
      "Training loss: 12.323039245605468\n",
      "Training loss: 13.286708068847656\n",
      "Training loss: 12.272353363037109\n",
      "Training loss: 13.228836059570312\n",
      "Training loss: 12.221876525878907\n",
      "Training loss: 13.171328735351562\n",
      "Training loss: 12.17165756225586\n",
      "Training loss: 13.114096069335938\n",
      "Training loss: 12.121622467041016\n",
      "Training loss: 13.057121276855469\n",
      "Training loss: 12.071883392333984\n",
      "Training loss: 13.000436401367187\n",
      "Training loss: 12.022353363037109\n",
      "Training loss: 12.944046020507812\n",
      "Training loss: 11.973040771484374\n",
      "Training loss: 12.887919616699218\n",
      "Training loss: 11.923954010009766\n",
      "Training loss: 12.832106018066407\n",
      "Training loss: 11.875203704833984\n",
      "Training loss: 12.776625061035157\n",
      "Training loss: 11.826652526855469\n",
      "Training loss: 12.721403503417969\n",
      "Training loss: 11.77828598022461\n",
      "Training loss: 12.666419982910156\n",
      "Training loss: 11.730112457275391\n",
      "Training loss: 12.611676025390626\n",
      "Training loss: 11.682197570800781\n",
      "Training loss: 12.557261657714843\n",
      "Training loss: 11.634538269042968\n",
      "Training loss: 12.503129577636718\n",
      "Training loss: 11.587028503417969\n",
      "Training loss: 12.44916000366211\n",
      "Training loss: 11.53972930908203\n",
      "Training loss: 12.395517730712891\n",
      "Training loss: 11.49264907836914\n",
      "Training loss: 12.342092132568359\n",
      "Training loss: 11.4457763671875\n",
      "Training loss: 12.28890380859375\n",
      "Training loss: 11.39910430908203\n",
      "Training loss: 12.23599624633789\n",
      "Training loss: 11.35271987915039\n",
      "Training loss: 12.183416748046875\n",
      "Training loss: 11.30651397705078\n",
      "Training loss: 12.131105041503906\n",
      "Training loss: 11.26059799194336\n",
      "Training loss: 12.079039001464844\n",
      "Training loss: 11.214847564697266\n",
      "Training loss: 12.027262115478516\n",
      "Training loss: 11.169352722167968\n",
      "Training loss: 11.975814056396484\n",
      "Training loss: 11.124147033691406\n",
      "Training loss: 11.924592590332031\n",
      "Training loss: 11.079080963134766\n",
      "Training loss: 11.873616790771484\n",
      "Training loss: 11.034226226806641\n",
      "Training loss: 11.82288818359375\n",
      "Training loss: 10.9895751953125\n",
      "Training loss: 11.77236099243164\n",
      "Training loss: 10.94510498046875\n",
      "Training loss: 11.722080993652344\n",
      "Training loss: 10.900810241699219\n",
      "Training loss: 11.672138977050782\n",
      "Training loss: 10.856884765625\n",
      "Training loss: 11.62247085571289\n",
      "Training loss: 10.813082122802735\n",
      "Training loss: 11.57300033569336\n",
      "Training loss: 10.76947021484375\n",
      "Training loss: 11.523787689208984\n",
      "Training loss: 10.726103210449219\n",
      "Training loss: 11.474809265136718\n",
      "Training loss: 10.682908630371093\n",
      "Training loss: 11.42612075805664\n",
      "Training loss: 10.640023040771485\n",
      "Training loss: 11.377699279785157\n",
      "Training loss: 10.597230529785156\n",
      "Training loss: 11.32947235107422\n",
      "Training loss: 10.554701232910157\n",
      "Training loss: 11.281508636474609\n",
      "Training loss: 10.512322998046875\n",
      "Training loss: 11.233758544921875\n",
      "Training loss: 10.47012939453125\n",
      "Training loss: 11.186167907714843\n",
      "Training loss: 10.428080749511718\n",
      "Training loss: 11.138808441162109\n",
      "Training loss: 10.38622817993164\n",
      "Training loss: 11.091752624511718\n",
      "Training loss: 10.344620513916016\n",
      "Training loss: 11.044914245605469\n",
      "Training loss: 10.303170013427735\n",
      "Training loss: 10.998211669921876\n",
      "Training loss: 10.26185302734375\n",
      "Training loss: 10.951759338378906\n",
      "Training loss: 10.220758819580078\n",
      "Training loss: 10.90556640625\n",
      "Training loss: 10.179966735839844\n",
      "Training loss: 10.859727478027343\n",
      "Training loss: 10.139334869384765\n",
      "Training loss: 10.814033508300781\n",
      "Training loss: 10.098918151855468\n",
      "Training loss: 10.768624114990235\n",
      "Training loss: 10.058613586425782\n",
      "Training loss: 10.723321533203125\n",
      "Training loss: 10.018470001220702\n",
      "Training loss: 10.678253173828125\n",
      "Training loss: 9.978597259521484\n",
      "Training loss: 10.633552551269531\n",
      "Training loss: 9.938959503173828\n",
      "Training loss: 10.58907699584961\n",
      "Training loss: 9.899519348144532\n",
      "Training loss: 10.54474105834961\n",
      "Training loss: 9.860198974609375\n",
      "Training loss: 10.500656890869141\n",
      "Training loss: 9.82104034423828\n",
      "Training loss: 10.456746673583984\n",
      "Training loss: 9.78209228515625\n",
      "Training loss: 10.413066101074218\n",
      "Training loss: 9.743327331542968\n",
      "Training loss: 10.369708251953124\n",
      "Training loss: 9.704841613769531\n",
      "Training loss: 10.326517486572266\n",
      "Training loss: 9.666391754150391\n",
      "Training loss: 10.283476257324219\n",
      "Training loss: 9.628188323974609\n",
      "Training loss: 10.24072723388672\n",
      "Training loss: 9.590237426757813\n",
      "Training loss: 10.198180389404296\n",
      "Training loss: 9.552342987060547\n",
      "Training loss: 10.155776977539062\n",
      "Training loss: 9.514667510986328\n",
      "Training loss: 10.11365203857422\n",
      "Training loss: 9.47716827392578\n",
      "Training loss: 10.07175064086914\n",
      "Training loss: 9.439901733398438\n",
      "Training loss: 10.030015563964843\n",
      "Training loss: 9.402664947509766\n",
      "Training loss: 9.988379669189452\n",
      "Training loss: 9.365620422363282\n",
      "Training loss: 9.947047424316406\n",
      "Training loss: 9.328871154785157\n",
      "Training loss: 9.905982971191406\n",
      "Training loss: 9.292283630371093\n",
      "Training loss: 9.865097045898438\n",
      "Training loss: 9.25586166381836\n",
      "Training loss: 9.824430847167969\n",
      "Training loss: 9.219595336914063\n",
      "Training loss: 9.783956146240234\n",
      "Training loss: 9.183464813232423\n",
      "Training loss: 9.743617248535156\n",
      "Training loss: 9.147512054443359\n",
      "Training loss: 9.703516387939453\n",
      "Training loss: 9.111691284179688\n",
      "Training loss: 9.663565826416015\n",
      "Training loss: 9.076085662841797\n",
      "Training loss: 9.62389678955078\n",
      "Training loss: 9.040724182128907\n",
      "Training loss: 9.584421539306641\n",
      "Training loss: 9.005464172363281\n",
      "Training loss: 9.54513168334961\n",
      "Training loss: 8.970388793945313\n",
      "Training loss: 9.50610122680664\n",
      "Training loss: 8.93548812866211\n",
      "Training loss: 9.467223358154296\n",
      "Training loss: 8.900736236572266\n",
      "Training loss: 9.428498077392579\n",
      "Training loss: 8.866102600097657\n",
      "Training loss: 9.389906311035157\n",
      "Training loss: 8.831614685058593\n",
      "Training loss: 9.351557159423828\n",
      "Training loss: 8.797329711914063\n",
      "Training loss: 9.313420867919922\n",
      "Training loss: 8.763272094726563\n",
      "Training loss: 9.275520324707031\n",
      "Training loss: 8.72933120727539\n",
      "Training loss: 9.237749481201172\n",
      "Training loss: 8.695512390136718\n",
      "Training loss: 9.200187683105469\n",
      "Training loss: 8.661837768554687\n",
      "Training loss: 9.162718200683594\n",
      "Training loss: 8.628369140625\n",
      "Training loss: 9.125617980957031\n",
      "Training loss: 8.595096588134766\n",
      "Training loss: 9.088607025146484\n",
      "Training loss: 8.561939239501953\n",
      "Training loss: 9.051788330078125\n",
      "Training loss: 8.528965759277344\n",
      "Training loss: 9.015190887451173\n",
      "Training loss: 8.49612808227539\n",
      "Training loss: 8.978710174560547\n",
      "Training loss: 8.463441467285156\n",
      "Training loss: 8.942469787597656\n",
      "Training loss: 8.430915832519531\n",
      "Training loss: 8.906385040283203\n",
      "Training loss: 8.398583221435548\n",
      "Training loss: 8.870539093017578\n",
      "Training loss: 8.366357421875\n",
      "Training loss: 8.834785461425781\n",
      "Training loss: 8.334269714355468\n",
      "Training loss: 8.79925308227539\n",
      "Training loss: 8.302444458007812\n",
      "Training loss: 8.763922882080077\n",
      "Training loss: 8.270692443847656\n",
      "Training loss: 8.728759002685546\n",
      "Training loss: 8.239070892333984\n",
      "Training loss: 8.693712615966797\n",
      "Training loss: 8.20758285522461\n",
      "Training loss: 8.658866882324219\n",
      "Training loss: 8.176231384277344\n",
      "Training loss: 8.624119567871094\n",
      "Training loss: 8.145004272460938\n",
      "Training loss: 8.58960418701172\n",
      "Training loss: 8.113953399658204\n",
      "Training loss: 8.555193328857422\n",
      "Training loss: 8.08306884765625\n",
      "Training loss: 8.52106170654297\n",
      "Training loss: 8.052362060546875\n",
      "Training loss: 8.487110900878907\n",
      "Training loss: 8.021784210205078\n",
      "Training loss: 8.453235626220703\n",
      "Training loss: 7.991293334960938\n",
      "Training loss: 8.419601440429688\n",
      "Training loss: 7.961025238037109\n",
      "Training loss: 8.386180877685547\n",
      "Training loss: 7.930938720703125\n",
      "Training loss: 8.352906799316406\n",
      "Training loss: 7.9009452819824215\n",
      "Training loss: 8.319762420654296\n",
      "Training loss: 7.87109375\n",
      "Training loss: 8.286766052246094\n",
      "Training loss: 7.841343688964844\n",
      "Training loss: 8.253948974609376\n",
      "Training loss: 7.811763763427734\n",
      "Training loss: 8.221307373046875\n",
      "Training loss: 7.782307434082031\n",
      "Training loss: 8.188763427734376\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 7.752973175048828\n",
      "Training loss: 8.15645751953125\n",
      "Training loss: 7.723825836181641\n",
      "Training loss: 8.124296569824219\n",
      "Training loss: 7.694811248779297\n",
      "Training loss: 8.092298126220703\n",
      "Training loss: 7.665972900390625\n",
      "Training loss: 8.060514831542969\n",
      "Training loss: 7.6372734069824215\n",
      "Training loss: 8.028900909423829\n",
      "Training loss: 7.608653259277344\n",
      "Training loss: 7.9973396301269535\n",
      "Training loss: 7.5801643371582035\n",
      "Training loss: 7.965952301025391\n",
      "Training loss: 7.551844787597656\n",
      "Training loss: 7.934733581542969\n",
      "Training loss: 7.523548889160156\n",
      "Training loss: 7.903588104248047\n",
      "Training loss: 7.495445251464844\n",
      "Training loss: 7.872697448730468\n",
      "Training loss: 7.467555236816406\n",
      "Training loss: 7.841987609863281\n",
      "Training loss: 7.439765930175781\n",
      "Training loss: 7.811412811279297\n",
      "Training loss: 7.412095642089843\n",
      "Training loss: 7.780979156494141\n",
      "Training loss: 7.384532928466797\n",
      "Training loss: 7.750648498535156\n",
      "Training loss: 7.357076263427734\n",
      "Training loss: 7.7204231262207035\n",
      "Training loss: 7.329744720458985\n",
      "Training loss: 7.690425109863281\n",
      "Training loss: 7.302616882324219\n",
      "Training loss: 7.660594940185547\n",
      "Training loss: 7.27557601928711\n",
      "Training loss: 7.630921936035156\n",
      "Training loss: 7.2487030029296875\n",
      "Training loss: 7.601408386230469\n",
      "Training loss: 7.221949768066406\n",
      "Training loss: 7.572021484375\n",
      "Training loss: 7.195307922363281\n",
      "Training loss: 7.542765045166016\n",
      "Training loss: 7.168840026855468\n",
      "Training loss: 7.513701629638672\n",
      "Training loss: 7.142430114746094\n",
      "Training loss: 7.484709930419922\n",
      "Training loss: 7.116142272949219\n",
      "Training loss: 7.455894470214844\n",
      "Training loss: 7.089988708496094\n",
      "Training loss: 7.427202606201172\n",
      "Training loss: 7.063973999023437\n",
      "Training loss: 7.398673248291016\n",
      "Training loss: 7.0380859375\n",
      "Training loss: 7.370268249511719\n",
      "Training loss: 7.012284851074218\n",
      "Training loss: 7.342011260986328\n",
      "Training loss: 6.9866493225097654\n",
      "Training loss: 7.313948059082032\n",
      "Training loss: 6.961161804199219\n",
      "Training loss: 7.2859619140625\n",
      "Training loss: 6.935736083984375\n",
      "Training loss: 7.258121490478516\n",
      "Training loss: 6.910444641113282\n",
      "Training loss: 7.230419921875\n",
      "Training loss: 6.885282135009765\n",
      "Training loss: 7.202903747558594\n",
      "Training loss: 6.8602447509765625\n",
      "Training loss: 7.175507354736328\n",
      "Training loss: 6.835335540771484\n",
      "Training loss: 7.148240661621093\n",
      "Training loss: 6.810579681396485\n",
      "Training loss: 7.121162414550781\n",
      "Training loss: 6.785899353027344\n",
      "Training loss: 7.094159698486328\n",
      "Training loss: 6.761355590820313\n",
      "Training loss: 7.067308044433593\n",
      "Training loss: 6.736928558349609\n",
      "Training loss: 7.040620422363281\n",
      "Training loss: 6.712611389160156\n",
      "Training loss: 7.014024353027343\n",
      "Training loss: 6.688447570800781\n",
      "Training loss: 6.98761215209961\n",
      "Training loss: 6.664340972900391\n",
      "Training loss: 6.9613090515136715\n",
      "Training loss: 6.640431976318359\n",
      "Training loss: 6.935198211669922\n",
      "Training loss: 6.616593933105468\n",
      "Training loss: 6.9091064453125\n",
      "Training loss: 6.592814636230469\n",
      "Training loss: 6.883161926269532\n",
      "Training loss: 6.569159698486328\n",
      "Training loss: 6.857319641113281\n",
      "Training loss: 6.545647430419922\n",
      "Training loss: 6.8317115783691404\n",
      "Training loss: 6.522312164306641\n",
      "Training loss: 6.806232452392578\n",
      "Training loss: 6.49896469116211\n",
      "Training loss: 6.7807472229003904\n",
      "Training loss: 6.4757843017578125\n",
      "Training loss: 6.755451202392578\n",
      "Training loss: 6.4527137756347654\n",
      "Training loss: 6.730302429199218\n",
      "Training loss: 6.429740905761719\n",
      "Training loss: 6.705239105224609\n",
      "Training loss: 6.406841278076172\n",
      "Training loss: 6.6803230285644535\n",
      "Training loss: 6.38409194946289\n",
      "Training loss: 6.655476379394531\n",
      "Training loss: 6.36139144897461\n",
      "Training loss: 6.630793762207031\n",
      "Training loss: 6.338884735107422\n",
      "Training loss: 6.606268310546875\n",
      "Training loss: 6.316485595703125\n",
      "Training loss: 6.581884765625\n",
      "Training loss: 6.294217681884765\n",
      "Training loss: 6.557609558105469\n",
      "Training loss: 6.271973419189453\n",
      "Training loss: 6.533347320556641\n",
      "Training loss: 6.24981575012207\n",
      "Training loss: 6.50928955078125\n",
      "Training loss: 6.2277881622314455\n",
      "Training loss: 6.485375213623047\n",
      "Training loss: 6.2059581756591795\n",
      "Training loss: 6.461608123779297\n",
      "Training loss: 6.184236526489258\n",
      "Training loss: 6.4379737854003904\n",
      "Training loss: 6.162577438354492\n",
      "Training loss: 6.414418029785156\n",
      "Training loss: 6.141021728515625\n",
      "Training loss: 6.3909751892089846\n",
      "Training loss: 6.119527435302734\n",
      "Training loss: 6.367621994018554\n",
      "Training loss: 6.0981487274169925\n",
      "Training loss: 6.344390869140625\n",
      "Training loss: 6.07691535949707\n",
      "Training loss: 6.321270751953125\n",
      "Training loss: 6.055664825439453\n",
      "Training loss: 6.298259353637695\n",
      "Training loss: 6.03462028503418\n",
      "Training loss: 6.275368499755859\n",
      "Training loss: 6.013663482666016\n",
      "Training loss: 6.25261344909668\n",
      "Training loss: 5.9927726745605465\n",
      "Training loss: 6.2299449920654295\n",
      "Training loss: 5.971998977661133\n",
      "Training loss: 6.207400512695313\n",
      "Training loss: 5.9512779235839846\n",
      "Training loss: 6.1849018096923825\n",
      "Training loss: 5.930693817138672\n",
      "Training loss: 6.16258659362793\n",
      "Training loss: 5.910260009765625\n",
      "Training loss: 6.14039306640625\n",
      "Training loss: 5.889874267578125\n",
      "Training loss: 6.118323516845703\n",
      "Training loss: 5.869639205932617\n",
      "Training loss: 6.096408843994141\n",
      "Training loss: 5.8495330810546875\n",
      "Training loss: 6.074586868286133\n",
      "Training loss: 5.829430389404297\n",
      "Training loss: 6.052798461914063\n",
      "Training loss: 5.809519195556641\n",
      "Training loss: 6.031228637695312\n",
      "Training loss: 5.789635086059571\n",
      "Training loss: 6.009665679931641\n",
      "Training loss: 5.769846725463867\n",
      "Training loss: 5.988271331787109\n",
      "Training loss: 5.7501472473144535\n",
      "Training loss: 5.966901779174805\n",
      "Training loss: 5.7305347442626955\n",
      "Training loss: 5.9456626892089846\n",
      "Training loss: 5.711008834838867\n",
      "Training loss: 5.9245250701904295\n",
      "Training loss: 5.691569900512695\n",
      "Training loss: 5.90350112915039\n",
      "Training loss: 5.6722148895263675\n",
      "Training loss: 5.88255500793457\n",
      "Training loss: 5.65295524597168\n",
      "Training loss: 5.861725616455078\n",
      "Training loss: 5.6338356018066404\n",
      "Training loss: 5.841035079956055\n",
      "Training loss: 5.614736175537109\n",
      "Training loss: 5.820379638671875\n",
      "Training loss: 5.595780563354492\n",
      "Training loss: 5.799903488159179\n",
      "Training loss: 5.576927947998047\n",
      "Training loss: 5.779521942138672\n",
      "Training loss: 5.558136749267578\n",
      "Training loss: 5.759238815307617\n",
      "Training loss: 5.5394855499267575\n",
      "Training loss: 5.7390491485595705\n",
      "Training loss: 5.520866775512696\n",
      "Training loss: 5.71894416809082\n",
      "Training loss: 5.502393341064453\n",
      "Training loss: 5.698981475830078\n",
      "Training loss: 5.483954620361328\n",
      "Training loss: 5.679059982299805\n",
      "Training loss: 5.4655803680419925\n",
      "Training loss: 5.659234619140625\n",
      "Training loss: 5.447309494018555\n",
      "Training loss: 5.639518356323242\n",
      "Training loss: 5.4291942596435545\n",
      "Training loss: 5.620017242431641\n",
      "Training loss: 5.4111591339111325\n",
      "Training loss: 5.600494384765625\n",
      "Training loss: 5.393165969848633\n",
      "Training loss: 5.581119537353516\n",
      "Training loss: 5.375298309326172\n",
      "Training loss: 5.561842727661133\n",
      "Training loss: 5.357494354248047\n",
      "Training loss: 5.542633819580078\n",
      "Training loss: 5.339776611328125\n",
      "Training loss: 5.523520660400391\n",
      "Training loss: 5.322137451171875\n",
      "Training loss: 5.504514694213867\n",
      "Training loss: 5.3046211242675785\n",
      "Training loss: 5.485634613037109\n",
      "Training loss: 5.28715934753418\n",
      "Training loss: 5.466793823242187\n",
      "Training loss: 5.2697608947753904\n",
      "Training loss: 5.4480854034423825\n",
      "Training loss: 5.252484130859375\n",
      "Training loss: 5.42945556640625\n",
      "Training loss: 5.235237503051758\n",
      "Training loss: 5.410857391357422\n",
      "Training loss: 5.218084716796875\n",
      "Training loss: 5.392421722412109\n",
      "Training loss: 5.2010047912597654\n",
      "Training loss: 5.374031829833984\n",
      "Training loss: 5.184043502807617\n",
      "Training loss: 5.355782318115234\n",
      "Training loss: 5.167165374755859\n",
      "Training loss: 5.337624359130859\n",
      "Training loss: 5.150379180908203\n",
      "Training loss: 5.319559478759766\n",
      "Training loss: 5.1336627960205075\n",
      "Training loss: 5.3015789031982425\n",
      "Training loss: 5.117047882080078\n",
      "Training loss: 5.283710479736328\n",
      "Training loss: 5.100515747070313\n",
      "Training loss: 5.265906524658203\n",
      "Training loss: 5.084045791625977\n",
      "Training loss: 5.248213577270508\n",
      "Training loss: 5.067641830444336\n",
      "Training loss: 5.230580520629883\n",
      "Training loss: 5.0513252258300785\n",
      "Training loss: 5.2130474090576175\n",
      "Training loss: 5.035060119628906\n",
      "Training loss: 5.195569610595703\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 5.018890762329102\n",
      "Training loss: 5.178174591064453\n",
      "Training loss: 5.002782821655273\n",
      "Training loss: 5.160880279541016\n",
      "Training loss: 4.986782073974609\n",
      "Training loss: 5.143688201904297\n",
      "Training loss: 4.970838165283203\n",
      "Training loss: 5.126549148559571\n",
      "Training loss: 4.954955291748047\n",
      "Training loss: 5.109529876708985\n",
      "Training loss: 4.939177703857422\n",
      "Training loss: 5.092580795288086\n",
      "Training loss: 4.923435211181641\n",
      "Training loss: 5.075723266601562\n",
      "Training loss: 4.907839965820313\n",
      "Training loss: 5.058959197998047\n",
      "Training loss: 4.892330169677734\n",
      "Training loss: 5.042334747314453\n",
      "Training loss: 4.87684440612793\n",
      "Training loss: 5.025719833374024\n",
      "Training loss: 4.861466979980468\n",
      "Training loss: 5.0092521667480465\n",
      "Training loss: 4.84615478515625\n",
      "Training loss: 4.992815399169922\n",
      "Training loss: 4.8308860778808596\n",
      "Training loss: 4.976443099975586\n",
      "Training loss: 4.8157188415527346\n",
      "Training loss: 4.96019287109375\n",
      "Training loss: 4.800609588623047\n",
      "Training loss: 4.9439697265625\n",
      "Training loss: 4.7855266571044925\n",
      "Training loss: 4.92784538269043\n",
      "Training loss: 4.770592117309571\n",
      "Training loss: 4.911835098266602\n",
      "Training loss: 4.755713272094726\n",
      "Training loss: 4.895883941650391\n",
      "Training loss: 4.740886688232422\n",
      "Training loss: 4.88001708984375\n",
      "Training loss: 4.726117706298828\n",
      "Training loss: 4.864207077026367\n",
      "Training loss: 4.711452865600586\n",
      "Training loss: 4.84849967956543\n",
      "Training loss: 4.696848297119141\n",
      "Training loss: 4.832876968383789\n",
      "Training loss: 4.682330322265625\n",
      "Training loss: 4.81734619140625\n",
      "Training loss: 4.667868804931641\n",
      "Training loss: 4.80185546875\n",
      "Training loss: 4.653459930419922\n",
      "Training loss: 4.786432266235352\n",
      "Training loss: 4.639105987548828\n",
      "Training loss: 4.771111297607422\n",
      "Training loss: 4.6248432159423825\n",
      "Training loss: 4.755806732177734\n",
      "Training loss: 4.61064567565918\n",
      "Training loss: 4.740684890747071\n",
      "Training loss: 4.596564102172851\n",
      "Training loss: 4.725628662109375\n",
      "Training loss: 4.582527923583984\n",
      "Training loss: 4.710615921020508\n",
      "Training loss: 4.568549728393554\n",
      "Training loss: 4.695649719238281\n",
      "Training loss: 4.554599380493164\n",
      "Training loss: 4.680764007568359\n",
      "Training loss: 4.540743255615235\n",
      "Training loss: 4.665925216674805\n",
      "Training loss: 4.526930236816407\n",
      "Training loss: 4.651203918457031\n",
      "Training loss: 4.5132499694824215\n",
      "Training loss: 4.63659553527832\n",
      "Training loss: 4.49958381652832\n",
      "Training loss: 4.621974182128906\n",
      "Training loss: 4.485934829711914\n",
      "Training loss: 4.607389068603515\n",
      "Training loss: 4.472394561767578\n",
      "Training loss: 4.592969512939453\n",
      "Training loss: 4.458933639526367\n",
      "Training loss: 4.578586196899414\n",
      "Training loss: 4.445503997802734\n",
      "Training loss: 4.564261245727539\n",
      "Training loss: 4.432166671752929\n",
      "Training loss: 4.550052642822266\n",
      "Training loss: 4.418922424316406\n",
      "Training loss: 4.535888290405273\n",
      "Training loss: 4.405704116821289\n",
      "Training loss: 4.521807479858398\n",
      "Training loss: 4.392565536499023\n",
      "Training loss: 4.507787704467773\n",
      "Training loss: 4.379465484619141\n",
      "Training loss: 4.493829345703125\n",
      "Training loss: 4.366450500488281\n",
      "Training loss: 4.479970550537109\n",
      "Training loss: 4.3535198211669925\n",
      "Training loss: 4.466208648681641\n",
      "Training loss: 4.3406623840332035\n",
      "Training loss: 4.452491378784179\n",
      "Training loss: 4.3278755187988285\n",
      "Training loss: 4.438848495483398\n",
      "Training loss: 4.315102005004883\n",
      "Training loss: 4.425230407714844\n",
      "Training loss: 4.302394104003906\n",
      "Training loss: 4.4116863250732425\n",
      "Training loss: 4.289718246459961\n",
      "Training loss: 4.398166656494141\n",
      "Training loss: 4.277095794677734\n",
      "Training loss: 4.38476791381836\n",
      "Training loss: 4.264567184448242\n",
      "Training loss: 4.3714347839355465\n",
      "Training loss: 4.252108001708985\n",
      "Training loss: 4.358152770996094\n",
      "Training loss: 4.2397003173828125\n",
      "Training loss: 4.344948196411133\n",
      "Training loss: 4.227346420288086\n",
      "Training loss: 4.3318122863769535\n",
      "Training loss: 4.215063095092773\n",
      "Training loss: 4.31873779296875\n",
      "Training loss: 4.202857589721679\n",
      "Training loss: 4.305794143676758\n",
      "Training loss: 4.190707015991211\n",
      "Training loss: 4.2928516387939455\n",
      "Training loss: 4.178580474853516\n",
      "Training loss: 4.2799629211425785\n",
      "Training loss: 4.1665809631347654\n",
      "Training loss: 4.267203903198242\n",
      "Training loss: 4.154624557495117\n",
      "Training loss: 4.254508209228516\n",
      "Training loss: 4.142723083496094\n",
      "Training loss: 4.241851806640625\n",
      "Training loss: 4.130899047851562\n",
      "Training loss: 4.229238510131836\n",
      "Training loss: 4.119053649902344\n",
      "Training loss: 4.216680908203125\n",
      "Training loss: 4.107332611083985\n",
      "Training loss: 4.204211807250976\n",
      "Training loss: 4.095619964599609\n",
      "Training loss: 4.191765594482422\n",
      "Training loss: 4.083940887451172\n",
      "Training loss: 4.179347991943359\n",
      "Training loss: 4.072359848022461\n",
      "Training loss: 4.167097091674805\n",
      "Training loss: 4.060880279541015\n",
      "Training loss: 4.154885101318359\n",
      "Training loss: 4.049399566650391\n",
      "Training loss: 4.142672729492188\n",
      "Training loss: 4.037970733642578\n",
      "Training loss: 4.130562591552734\n",
      "Training loss: 4.026600646972656\n",
      "Training loss: 4.118495178222656\n",
      "Training loss: 4.0153343200683596\n",
      "Training loss: 4.106561660766602\n",
      "Training loss: 4.00410041809082\n",
      "Training loss: 4.094622802734375\n",
      "Training loss: 3.992898941040039\n",
      "Training loss: 4.082720947265625\n",
      "Training loss: 3.981755828857422\n",
      "Training loss: 4.070918273925781\n",
      "Training loss: 3.970650863647461\n",
      "Training loss: 4.059136581420899\n",
      "Training loss: 3.9595970153808593\n",
      "Training loss: 4.047416305541992\n",
      "Training loss: 3.9486392974853515\n",
      "Training loss: 4.035785675048828\n",
      "Training loss: 3.937667465209961\n",
      "Training loss: 4.0241554260253904\n",
      "Training loss: 3.9267845153808594\n",
      "Training loss: 4.012656402587891\n",
      "Training loss: 3.916000747680664\n",
      "Training loss: 4.001210784912109\n",
      "Training loss: 3.905220794677734\n",
      "Training loss: 3.9897716522216795\n",
      "Training loss: 3.8944801330566405\n",
      "Training loss: 3.9784080505371096\n",
      "Training loss: 3.8838096618652345\n",
      "Training loss: 3.967105484008789\n",
      "Training loss: 3.8731788635253905\n",
      "Training loss: 3.955844497680664\n",
      "Training loss: 3.862599563598633\n",
      "Training loss: 3.9446434020996093\n",
      "Training loss: 3.852094268798828\n",
      "Training loss: 3.933510589599609\n",
      "Training loss: 3.8416362762451173\n",
      "Training loss: 3.9224498748779295\n",
      "Training loss: 3.8312705993652343\n",
      "Training loss: 3.9115016937255858\n",
      "Training loss: 3.820906066894531\n",
      "Training loss: 3.900483322143555\n",
      "Training loss: 3.8105518341064455\n",
      "Training loss: 3.889528274536133\n",
      "Training loss: 3.800243377685547\n",
      "Training loss: 3.8786544799804688\n",
      "Training loss: 3.7900154113769533\n",
      "Training loss: 3.8678245544433594\n",
      "Training loss: 3.7798385620117188\n",
      "Training loss: 3.8570594787597656\n",
      "Training loss: 3.7697120666503907\n",
      "Training loss: 3.84635009765625\n",
      "Training loss: 3.759648895263672\n",
      "Training loss: 3.8357189178466795\n",
      "Training loss: 3.7496341705322265\n",
      "Training loss: 3.8251609802246094\n",
      "Training loss: 3.7396984100341797\n",
      "Training loss: 3.8146377563476563\n",
      "Training loss: 3.7297840118408203\n",
      "Training loss: 3.804149627685547\n",
      "Training loss: 3.719919967651367\n",
      "Training loss: 3.793722152709961\n",
      "Training loss: 3.710099792480469\n",
      "Training loss: 3.7833423614501953\n",
      "Training loss: 3.700322723388672\n",
      "Training loss: 3.7730087280273437\n",
      "Training loss: 3.690587615966797\n",
      "Training loss: 3.7627185821533202\n",
      "Training loss: 3.680904006958008\n",
      "Training loss: 3.7525062561035156\n",
      "Training loss: 3.671297073364258\n",
      "Training loss: 3.7423721313476563\n",
      "Training loss: 3.661690902709961\n",
      "Training loss: 3.73221435546875\n",
      "Training loss: 3.652130889892578\n",
      "Training loss: 3.7221141815185548\n",
      "Training loss: 3.642609405517578\n",
      "Training loss: 3.712067413330078\n",
      "Training loss: 3.6331413269042967\n",
      "Training loss: 3.7020797729492188\n",
      "Training loss: 3.623760986328125\n",
      "Training loss: 3.692168426513672\n",
      "Training loss: 3.614389419555664\n",
      "Training loss: 3.6822841644287108\n",
      "Training loss: 3.605056381225586\n",
      "Training loss: 3.672434616088867\n",
      "Training loss: 3.5957805633544924\n",
      "Training loss: 3.6626556396484373\n",
      "Training loss: 3.586528778076172\n",
      "Training loss: 3.6529090881347654\n",
      "Training loss: 3.577370452880859\n",
      "Training loss: 3.6432384490966796\n",
      "Training loss: 3.568247604370117\n",
      "Training loss: 3.6336299896240236\n",
      "Training loss: 3.559146499633789\n",
      "Training loss: 3.6240211486816407\n",
      "Training loss: 3.550101089477539\n",
      "Training loss: 3.614493560791016\n",
      "Training loss: 3.5410797119140627\n",
      "Training loss: 3.604994201660156\n",
      "Training loss: 3.532140350341797\n",
      "Training loss: 3.595574951171875\n",
      "Training loss: 3.5232372283935547\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 3.586167907714844\n",
      "Training loss: 3.5143287658691404\n",
      "Training loss: 3.5767902374267577\n",
      "Training loss: 3.505481719970703\n",
      "Training loss: 3.5674888610839846\n",
      "Training loss: 3.4967269897460938\n",
      "Training loss: 3.5582660675048827\n",
      "Training loss: 3.4879852294921876\n",
      "Training loss: 3.549054718017578\n",
      "Training loss: 3.4793022155761717\n",
      "Training loss: 3.5399024963378904\n",
      "Training loss: 3.4706256866455076\n",
      "Training loss: 3.53077392578125\n",
      "Training loss: 3.4620479583740233\n",
      "Training loss: 3.521736907958984\n",
      "Training loss: 3.4534576416015623\n",
      "Training loss: 3.5126930236816407\n",
      "Training loss: 3.444933319091797\n",
      "Training loss: 3.5036998748779298\n",
      "Training loss: 3.436403274536133\n",
      "Training loss: 3.4947662353515625\n",
      "Training loss: 3.427956390380859\n",
      "Training loss: 3.485872268676758\n",
      "Training loss: 3.419557571411133\n",
      "Training loss: 3.4770397186279296\n",
      "Training loss: 3.411235046386719\n",
      "Training loss: 3.468272018432617\n",
      "Training loss: 3.402862548828125\n",
      "Training loss: 3.459459686279297\n",
      "Training loss: 3.394562911987305\n",
      "Training loss: 3.4507476806640627\n",
      "Training loss: 3.386332702636719\n",
      "Training loss: 3.4420806884765627\n",
      "Training loss: 3.3781047821044923\n",
      "Training loss: 3.433432769775391\n",
      "Training loss: 3.369928741455078\n",
      "Training loss: 3.424854278564453\n",
      "Training loss: 3.361810302734375\n",
      "Training loss: 3.416286087036133\n",
      "Training loss: 3.3536914825439452\n",
      "Training loss: 3.40777587890625\n",
      "Training loss: 3.3456310272216796\n",
      "Training loss: 3.3993080139160154\n",
      "Training loss: 3.3376251220703126\n",
      "Training loss: 3.390882110595703\n",
      "Training loss: 3.3296417236328124\n",
      "Training loss: 3.3825382232666015\n",
      "Training loss: 3.3217323303222654\n",
      "Training loss: 3.374205780029297\n",
      "Training loss: 3.3138454437255858\n",
      "Training loss: 3.365937042236328\n",
      "Training loss: 3.306017303466797\n",
      "Training loss: 3.3577014923095705\n",
      "Training loss: 3.29820556640625\n",
      "Training loss: 3.349486541748047\n",
      "Training loss: 3.2904224395751953\n",
      "Training loss: 3.341312026977539\n",
      "Training loss: 3.282648468017578\n",
      "Training loss: 3.3331417083740233\n",
      "Training loss: 3.2749515533447267\n",
      "Training loss: 3.3251060485839843\n",
      "Training loss: 3.2673080444335936\n",
      "Training loss: 3.317057418823242\n",
      "Training loss: 3.2596946716308595\n",
      "Training loss: 3.30908088684082\n",
      "Training loss: 3.2521080017089843\n",
      "Training loss: 3.3011138916015623\n",
      "Training loss: 3.244528961181641\n",
      "Training loss: 3.293162155151367\n",
      "Training loss: 3.2369915008544923\n",
      "Training loss: 3.285259246826172\n",
      "Training loss: 3.229503631591797\n",
      "Training loss: 3.277393341064453\n",
      "Training loss: 3.2220294952392576\n",
      "Training loss: 3.269556427001953\n",
      "Training loss: 3.214598846435547\n",
      "Training loss: 3.261766815185547\n",
      "Training loss: 3.207217788696289\n",
      "Training loss: 3.254024887084961\n",
      "Training loss: 3.1998693466186525\n",
      "Training loss: 3.2463302612304688\n",
      "Training loss: 3.1925830841064453\n",
      "Training loss: 3.238688278198242\n",
      "Training loss: 3.1853084564208984\n",
      "Training loss: 3.231083297729492\n",
      "Training loss: 3.178082275390625\n",
      "Training loss: 3.2234703063964845\n",
      "Training loss: 3.17086238861084\n",
      "Training loss: 3.2159313201904296\n",
      "Training loss: 3.163666915893555\n",
      "Training loss: 3.2083965301513673\n",
      "Training loss: 3.156551551818848\n",
      "Training loss: 3.200931167602539\n",
      "Training loss: 3.149481201171875\n",
      "Training loss: 3.1935367584228516\n",
      "Training loss: 3.1424371719360353\n",
      "Training loss: 3.1861698150634767\n",
      "Training loss: 3.135410118103027\n",
      "Training loss: 3.1787784576416014\n",
      "Training loss: 3.1283966064453126\n",
      "Training loss: 3.1714456558227537\n",
      "Training loss: 3.1214107513427733\n",
      "Training loss: 3.1641286849975585\n",
      "Training loss: 3.11447696685791\n",
      "Training loss: 3.1568838119506837\n",
      "Training loss: 3.107589340209961\n",
      "Training loss: 3.1496603012084963\n",
      "Training loss: 3.1007055282592773\n",
      "Training loss: 3.14248104095459\n",
      "Training loss: 3.0939117431640626\n",
      "Training loss: 3.13536376953125\n",
      "Training loss: 3.0871150970458983\n",
      "Training loss: 3.1282413482666014\n",
      "Training loss: 3.0803600311279298\n",
      "Training loss: 3.1212017059326174\n",
      "Training loss: 3.0736703872680664\n",
      "Training loss: 3.1141794204711912\n",
      "Training loss: 3.06697998046875\n",
      "Training loss: 3.107193946838379\n",
      "Training loss: 3.060309410095215\n",
      "Training loss: 3.1002056121826174\n",
      "Training loss: 3.0536996841430666\n",
      "Training loss: 3.0933069229125976\n",
      "Training loss: 3.047129821777344\n",
      "Training loss: 3.0864452362060546\n",
      "Training loss: 3.0405834197998045\n",
      "Training loss: 3.0795888900756836\n",
      "Training loss: 3.034046936035156\n",
      "Training loss: 3.072774124145508\n",
      "Training loss: 3.0275623321533205\n",
      "Training loss: 3.065974998474121\n",
      "Training loss: 3.021079444885254\n",
      "Training loss: 3.05917911529541\n",
      "Training loss: 3.014626121520996\n",
      "Training loss: 3.0524673461914062\n",
      "Training loss: 3.008250045776367\n",
      "Training loss: 3.0458042144775392\n",
      "Training loss: 3.001887893676758\n",
      "Training loss: 3.0391414642333983\n",
      "Training loss: 2.9955209732055663\n",
      "Training loss: 3.0325063705444335\n",
      "Training loss: 2.9892181396484374\n",
      "Training loss: 3.0259140014648436\n",
      "Training loss: 2.9829326629638673\n",
      "Training loss: 3.0193288803100584\n",
      "Training loss: 2.976645851135254\n",
      "Training loss: 3.0127864837646485\n",
      "Training loss: 2.9704153060913088\n",
      "Training loss: 3.0062789916992188\n",
      "Training loss: 2.964229774475098\n",
      "Training loss: 2.9998310089111326\n",
      "Training loss: 2.958085060119629\n",
      "Training loss: 2.9934228897094726\n",
      "Training loss: 2.9519573211669923\n",
      "Training loss: 2.987003707885742\n",
      "Training loss: 2.9458215713500975\n",
      "Training loss: 2.980615234375\n",
      "Training loss: 2.9397598266601563\n",
      "Training loss: 2.974297523498535\n",
      "Training loss: 2.9337238311767577\n",
      "Training loss: 2.967976379394531\n",
      "Training loss: 2.9277048110961914\n",
      "Training loss: 2.9617137908935547\n",
      "Training loss: 2.9217113494873046\n",
      "Training loss: 2.9554559707641603\n",
      "Training loss: 2.915760803222656\n",
      "Training loss: 2.9492609024047853\n",
      "Training loss: 2.909857749938965\n",
      "Training loss: 2.943088912963867\n",
      "Training loss: 2.903961181640625\n",
      "Training loss: 2.9369293212890626\n",
      "Training loss: 2.8980615615844725\n",
      "Training loss: 2.9307846069335937\n",
      "Training loss: 2.8922250747680662\n",
      "Training loss: 2.9246944427490233\n",
      "Training loss: 2.88641242980957\n",
      "Training loss: 2.918661880493164\n",
      "Training loss: 2.88065185546875\n",
      "Training loss: 2.912626266479492\n",
      "Training loss: 2.8748910903930662\n",
      "Training loss: 2.9066320419311524\n",
      "Training loss: 2.8691455841064455\n",
      "Training loss: 2.9006343841552735\n",
      "Training loss: 2.8634466171264648\n",
      "Training loss: 2.8947116851806642\n",
      "Training loss: 2.857792091369629\n",
      "Training loss: 2.8887964248657227\n",
      "Training loss: 2.8521167755126955\n",
      "Training loss: 2.882914161682129\n",
      "Training loss: 2.846510887145996\n",
      "Training loss: 2.8770532608032227\n",
      "Training loss: 2.8409032821655273\n",
      "Training loss: 2.8712215423583984\n",
      "Training loss: 2.835336685180664\n",
      "Training loss: 2.8654338836669924\n",
      "Training loss: 2.829799842834473\n",
      "Training loss: 2.859659957885742\n",
      "Training loss: 2.8242931365966797\n",
      "Training loss: 2.8539342880249023\n",
      "Training loss: 2.8188236236572264\n",
      "Training loss: 2.8482345581054687\n",
      "Training loss: 2.8133899688720705\n",
      "Training loss: 2.842568588256836\n",
      "Training loss: 2.8079662322998047\n",
      "Training loss: 2.8369348526000975\n",
      "Training loss: 2.802581214904785\n",
      "Training loss: 2.831318664550781\n",
      "Training loss: 2.7971878051757812\n",
      "Training loss: 2.8257266998291017\n",
      "Training loss: 2.791848373413086\n",
      "Training loss: 2.82015323638916\n",
      "Training loss: 2.786505126953125\n",
      "Training loss: 2.81458740234375\n",
      "Training loss: 2.781187057495117\n",
      "Training loss: 2.8090694427490233\n",
      "Training loss: 2.775909423828125\n",
      "Training loss: 2.8035842895507814\n",
      "Training loss: 2.7706939697265627\n",
      "Training loss: 2.798146629333496\n",
      "Training loss: 2.7654794692993163\n",
      "Training loss: 2.79272346496582\n",
      "Training loss: 2.760284996032715\n",
      "Training loss: 2.7873327255249025\n",
      "Training loss: 2.755130577087402\n",
      "Training loss: 2.7819616317749025\n",
      "Training loss: 2.7499973297119142\n",
      "Training loss: 2.7766204833984376\n",
      "Training loss: 2.744867515563965\n",
      "Training loss: 2.7712818145751954\n",
      "Training loss: 2.739767074584961\n",
      "Training loss: 2.76599063873291\n",
      "Training loss: 2.7347192764282227\n",
      "Training loss: 2.760740280151367\n",
      "Training loss: 2.729664611816406\n",
      "Training loss: 2.7554885864257814\n",
      "Training loss: 2.7246288299560546\n",
      "Training loss: 2.7502666473388673\n",
      "Training loss: 2.719651222229004\n",
      "Training loss: 2.7450641632080077\n",
      "Training loss: 2.714644432067871\n",
      "Training loss: 2.739889144897461\n",
      "Training loss: 2.7097143173217773\n",
      "Training loss: 2.734759521484375\n",
      "Training loss: 2.7047887802124024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 2.729638671875\n",
      "Training loss: 2.6998767852783203\n",
      "Training loss: 2.724538230895996\n",
      "Training loss: 2.6949899673461912\n",
      "Training loss: 2.719475746154785\n",
      "Training loss: 2.6901546478271485\n",
      "Training loss: 2.7144336700439453\n",
      "Training loss: 2.6853281021118165\n",
      "Training loss: 2.7094369888305665\n",
      "Training loss: 2.680531311035156\n",
      "Training loss: 2.7044668197631836\n",
      "Training loss: 2.6757699966430666\n",
      "Training loss: 2.6995098114013674\n",
      "Training loss: 2.6710281372070312\n",
      "Training loss: 2.6945873260498048\n",
      "Training loss: 2.666300964355469\n",
      "Training loss: 2.6896820068359375\n",
      "Training loss: 2.6616039276123047\n",
      "Training loss: 2.68480224609375\n",
      "Training loss: 2.656919479370117\n",
      "Training loss: 2.679946517944336\n",
      "Training loss: 2.6522409439086916\n",
      "Training loss: 2.675075912475586\n",
      "Training loss: 2.6475854873657227\n",
      "Training loss: 2.670256423950195\n",
      "Training loss: 2.642978286743164\n",
      "Training loss: 2.665471839904785\n",
      "Training loss: 2.6383716583251955\n",
      "Training loss: 2.6606958389282225\n",
      "Training loss: 2.6338180541992187\n",
      "Training loss: 2.655971717834473\n",
      "Training loss: 2.629258728027344\n",
      "Training loss: 2.651249122619629\n",
      "Training loss: 2.624717140197754\n",
      "Training loss: 2.6465581893920898\n",
      "Training loss: 2.6202220916748047\n",
      "Training loss: 2.641890525817871\n",
      "Training loss: 2.6157451629638673\n",
      "Training loss: 2.6372488021850584\n",
      "Training loss: 2.611288833618164\n",
      "Training loss: 2.632602882385254\n",
      "Training loss: 2.606868934631348\n",
      "Training loss: 2.628041648864746\n",
      "Training loss: 2.6024654388427733\n",
      "Training loss: 2.623454284667969\n",
      "Training loss: 2.598077964782715\n",
      "Training loss: 2.6189205169677736\n",
      "Training loss: 2.593711280822754\n",
      "Training loss: 2.6143930435180662\n",
      "Training loss: 2.589347267150879\n",
      "Training loss: 2.6098773956298826\n",
      "Training loss: 2.5850173950195314\n",
      "Training loss: 2.6053695678710938\n",
      "Training loss: 2.5806859970092773\n",
      "Training loss: 2.6008983612060548\n",
      "Training loss: 2.5763927459716798\n",
      "Training loss: 2.5964406967163085\n",
      "Training loss: 2.572098731994629\n",
      "Training loss: 2.59197940826416\n",
      "Training loss: 2.567814254760742\n",
      "Training loss: 2.587559127807617\n",
      "Training loss: 2.5635753631591798\n",
      "Training loss: 2.5831701278686525\n",
      "Training loss: 2.559382438659668\n",
      "Training loss: 2.5788259506225586\n",
      "Training loss: 2.5551929473876953\n",
      "Training loss: 2.574520492553711\n",
      "Training loss: 2.5510616302490234\n",
      "Training loss: 2.5702178955078123\n",
      "Training loss: 2.5469234466552733\n",
      "Training loss: 2.5659337997436524\n",
      "Training loss: 2.5428001403808596\n",
      "Training loss: 2.561666488647461\n",
      "Training loss: 2.5386726379394533\n",
      "Training loss: 2.557387351989746\n",
      "Training loss: 2.534581184387207\n",
      "Training loss: 2.5531503677368166\n",
      "Training loss: 2.5305364608764647\n",
      "Training loss: 2.5489566802978514\n",
      "Training loss: 2.526457977294922\n",
      "Training loss: 2.5447433471679686\n",
      "Training loss: 2.5224632263183593\n",
      "Training loss: 2.540629768371582\n",
      "Training loss: 2.5184652328491213\n",
      "Training loss: 2.536473846435547\n",
      "Training loss: 2.514456558227539\n",
      "Training loss: 2.5323230743408205\n",
      "Training loss: 2.5104846954345703\n",
      "Training loss: 2.5282260894775392\n",
      "Training loss: 2.5065364837646484\n",
      "Training loss: 2.5241313934326173\n",
      "Training loss: 2.5026033401489256\n",
      "Training loss: 2.5200740814208986\n",
      "Training loss: 2.4986751556396483\n",
      "Training loss: 2.516015815734863\n",
      "Training loss: 2.494782257080078\n",
      "Training loss: 2.511979866027832\n",
      "Training loss: 2.4909107208251955\n",
      "Training loss: 2.507978820800781\n",
      "Training loss: 2.487066078186035\n",
      "Training loss: 2.503999710083008\n",
      "Training loss: 2.483234405517578\n",
      "Training loss: 2.500065040588379\n",
      "Training loss: 2.4794452667236326\n",
      "Training loss: 2.496120834350586\n",
      "Training loss: 2.475651168823242\n",
      "Training loss: 2.4922183990478515\n",
      "Training loss: 2.471866416931152\n",
      "Training loss: 2.4882715225219725\n",
      "Training loss: 2.4680913925170898\n",
      "Training loss: 2.484391784667969\n",
      "Training loss: 2.4643695831298826\n",
      "Training loss: 2.4805641174316406\n",
      "Training loss: 2.4606601715087892\n",
      "Training loss: 2.476710891723633\n",
      "Training loss: 2.456930732727051\n",
      "Training loss: 2.472859573364258\n",
      "Training loss: 2.4532520294189455\n",
      "Training loss: 2.4690628051757812\n",
      "Training loss: 2.449589729309082\n",
      "Training loss: 2.4652727127075194\n",
      "Training loss: 2.4459465026855467\n",
      "Training loss: 2.461526298522949\n",
      "Training loss: 2.4423141479492188\n",
      "Training loss: 2.45776481628418\n",
      "Training loss: 2.4387184143066407\n",
      "Training loss: 2.45404167175293\n",
      "Training loss: 2.435110092163086\n",
      "Training loss: 2.450320816040039\n",
      "Training loss: 2.431525230407715\n",
      "Training loss: 2.446620559692383\n",
      "Training loss: 2.427949142456055\n",
      "Training loss: 2.442916679382324\n",
      "Training loss: 2.4243797302246093\n",
      "Training loss: 2.439236068725586\n",
      "Training loss: 2.420843315124512\n",
      "Training loss: 2.4355819702148436\n",
      "Training loss: 2.417326545715332\n",
      "Training loss: 2.4319507598876955\n",
      "Training loss: 2.4138193130493164\n",
      "Training loss: 2.4283504486083984\n",
      "Training loss: 2.410361099243164\n",
      "Training loss: 2.4247745513916015\n",
      "Training loss: 2.406909942626953\n",
      "Training loss: 2.4212129592895506\n",
      "Training loss: 2.4034603118896483\n",
      "Training loss: 2.4176456451416017\n",
      "Training loss: 2.4000314712524413\n",
      "Training loss: 2.4141260147094727\n",
      "Training loss: 2.3966426849365234\n",
      "Training loss: 2.410603713989258\n",
      "Training loss: 2.393227767944336\n",
      "Training loss: 2.4070783615112306\n",
      "Training loss: 2.3898395538330077\n",
      "Training loss: 2.40360050201416\n",
      "Training loss: 2.3864641189575195\n",
      "Training loss: 2.4001239776611327\n",
      "Training loss: 2.3831382751464845\n",
      "Training loss: 2.3966863632202147\n",
      "Training loss: 2.3798126220703124\n",
      "Training loss: 2.3932415008544923\n",
      "Training loss: 2.3764928817749023\n",
      "Training loss: 2.389837074279785\n",
      "Training loss: 2.3731943130493165\n",
      "Training loss: 2.3864301681518554\n",
      "Training loss: 2.3699270248413087\n",
      "Training loss: 2.383081817626953\n",
      "Training loss: 2.366693115234375\n",
      "Training loss: 2.3797298431396485\n",
      "Training loss: 2.3634334564208985\n",
      "Training loss: 2.3763641357421874\n",
      "Training loss: 2.360220527648926\n",
      "Training loss: 2.373066520690918\n",
      "Training loss: 2.3570037841796876\n",
      "Training loss: 2.369752883911133\n",
      "Training loss: 2.353822135925293\n",
      "Training loss: 2.3664661407470704\n",
      "Training loss: 2.3506431579589844\n",
      "Training loss: 2.363179016113281\n",
      "Training loss: 2.3474584579467774\n",
      "Training loss: 2.3598962783813477\n",
      "Training loss: 2.344282531738281\n",
      "Training loss: 2.3566314697265627\n",
      "Training loss: 2.341135787963867\n",
      "Training loss: 2.3533962249755858\n",
      "Training loss: 2.338018608093262\n",
      "Training loss: 2.3501712799072267\n",
      "Training loss: 2.334900665283203\n",
      "Training loss: 2.3469480514526366\n",
      "Training loss: 2.331789016723633\n",
      "Training loss: 2.343766975402832\n",
      "Training loss: 2.3287326812744142\n",
      "Training loss: 2.3406166076660155\n",
      "Training loss: 2.325670051574707\n",
      "Training loss: 2.33746280670166\n",
      "Training loss: 2.32263298034668\n",
      "Training loss: 2.3343481063842773\n",
      "Training loss: 2.319606971740723\n",
      "Training loss: 2.331215476989746\n",
      "Training loss: 2.3165733337402346\n",
      "Training loss: 2.328080749511719\n",
      "Training loss: 2.3135583877563475\n",
      "Training loss: 2.325\n",
      "Training loss: 2.3105752944946287\n",
      "Training loss: 2.321920394897461\n",
      "Training loss: 2.3076095581054688\n",
      "Training loss: 2.3188629150390625\n",
      "Training loss: 2.3046615600585936\n",
      "Training loss: 2.3158382415771483\n",
      "Training loss: 2.301718521118164\n",
      "Training loss: 2.3128053665161135\n",
      "Training loss: 2.298812484741211\n",
      "Training loss: 2.309819984436035\n",
      "Training loss: 2.295907402038574\n",
      "Training loss: 2.3068204879760743\n",
      "Training loss: 2.2929977416992187\n",
      "Training loss: 2.303821563720703\n",
      "Training loss: 2.2900932312011717\n",
      "Training loss: 2.30084228515625\n",
      "Training loss: 2.287216567993164\n",
      "Training loss: 2.297881317138672\n",
      "Training loss: 2.284364128112793\n",
      "Training loss: 2.294944190979004\n",
      "Training loss: 2.2815027236938477\n",
      "Training loss: 2.2919912338256836\n",
      "Training loss: 2.2786725997924804\n",
      "Training loss: 2.289083480834961\n",
      "Training loss: 2.2758527755737306\n",
      "Training loss: 2.286184310913086\n",
      "Training loss: 2.273046112060547\n",
      "Training loss: 2.2832948684692385\n",
      "Training loss: 2.270235061645508\n",
      "Training loss: 2.2804210662841795\n",
      "Training loss: 2.2674713134765625\n",
      "Training loss: 2.277568054199219\n",
      "Training loss: 2.2647001266479494\n",
      "Training loss: 2.2747243881225585\n",
      "Training loss: 2.2619596481323243\n",
      "Training loss: 2.271901321411133\n",
      "Training loss: 2.2592113494873045\n",
      "Training loss: 2.2690874099731446\n",
      "Training loss: 2.2565082550048827\n",
      "Training loss: 2.2662984848022463\n",
      "Training loss: 2.253790855407715\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 2.2635051727294924\n",
      "Training loss: 2.251102828979492\n",
      "Training loss: 2.260744094848633\n",
      "Training loss: 2.2484210968017577\n",
      "Training loss: 2.2579723358154298\n",
      "Training loss: 2.245736312866211\n",
      "Training loss: 2.255241584777832\n",
      "Training loss: 2.243094062805176\n",
      "Training loss: 2.2525152206420898\n",
      "Training loss: 2.2404605865478517\n",
      "Training loss: 2.2498043060302733\n",
      "Training loss: 2.2378314971923827\n",
      "Training loss: 2.2470970153808594\n",
      "Training loss: 2.2352121353149412\n",
      "Training loss: 2.2444114685058594\n",
      "Training loss: 2.2325908660888674\n",
      "Training loss: 2.241725540161133\n",
      "Training loss: 2.230002212524414\n",
      "Training loss: 2.2390708923339844\n",
      "Training loss: 2.227437400817871\n",
      "Training loss: 2.2364282608032227\n",
      "Training loss: 2.224858856201172\n",
      "Training loss: 2.233776092529297\n",
      "Training loss: 2.222291946411133\n",
      "Training loss: 2.231143569946289\n",
      "Training loss: 2.219760704040527\n",
      "Training loss: 2.228535461425781\n",
      "Training loss: 2.2172321319580077\n",
      "Training loss: 2.2259504318237306\n",
      "Training loss: 2.214716148376465\n",
      "Training loss: 2.223368835449219\n",
      "Training loss: 2.212223434448242\n",
      "Training loss: 2.2208087921142576\n",
      "Training loss: 2.209727096557617\n",
      "Training loss: 2.2182416915893555\n",
      "Training loss: 2.2072383880615236\n",
      "Training loss: 2.2156734466552734\n",
      "Training loss: 2.204762268066406\n",
      "Training loss: 2.21313419342041\n",
      "Training loss: 2.2022945404052736\n",
      "Training loss: 2.2106044769287108\n",
      "Training loss: 2.1998334884643556\n",
      "Training loss: 2.2080921173095702\n",
      "Training loss: 2.197403907775879\n",
      "Training loss: 2.2055995941162108\n",
      "Training loss: 2.194997215270996\n",
      "Training loss: 2.2031177520751952\n",
      "Training loss: 2.1925750732421876\n",
      "Training loss: 2.2006568908691406\n",
      "Training loss: 2.1902019500732424\n",
      "Training loss: 2.198197364807129\n",
      "Training loss: 2.187805938720703\n",
      "Training loss: 2.1957460403442384\n",
      "Training loss: 2.185428237915039\n",
      "Training loss: 2.193303108215332\n",
      "Training loss: 2.183066177368164\n",
      "Training loss: 2.1908695220947267\n",
      "Training loss: 2.18070125579834\n",
      "Training loss: 2.1884565353393555\n",
      "Training loss: 2.1783687591552736\n",
      "Training loss: 2.186074447631836\n",
      "Training loss: 2.176059341430664\n",
      "Training loss: 2.183694076538086\n",
      "Training loss: 2.173757553100586\n",
      "Training loss: 2.1813446044921876\n",
      "Training loss: 2.1714611053466797\n",
      "Training loss: 2.178985023498535\n",
      "Training loss: 2.169169616699219\n",
      "Training loss: 2.1766330718994142\n",
      "Training loss: 2.1668928146362303\n",
      "Training loss: 2.1742937088012697\n",
      "Training loss: 2.164628791809082\n",
      "Training loss: 2.1719661712646485\n",
      "Training loss: 2.1623668670654297\n",
      "Training loss: 2.169655418395996\n",
      "Training loss: 2.160129356384277\n",
      "Training loss: 2.1673700332641603\n",
      "Training loss: 2.1579124450683596\n",
      "Training loss: 2.1650819778442383\n",
      "Training loss: 2.1556842803955076\n",
      "Training loss: 2.162806510925293\n",
      "Training loss: 2.1534704208374023\n",
      "Training loss: 2.1605323791503905\n",
      "Training loss: 2.1512664794921874\n",
      "Training loss: 2.1582645416259765\n",
      "Training loss: 2.149066352844238\n",
      "Training loss: 2.1560285568237303\n",
      "Training loss: 2.146884536743164\n",
      "Training loss: 2.153781509399414\n",
      "Training loss: 2.1447116851806642\n",
      "Training loss: 2.1515548706054686\n",
      "Training loss: 2.142540168762207\n",
      "Training loss: 2.1493242263793944\n",
      "Training loss: 2.1403854370117186\n",
      "Training loss: 2.1471254348754885\n",
      "Training loss: 2.138226318359375\n",
      "Training loss: 2.144918441772461\n",
      "Training loss: 2.1361013412475587\n",
      "Training loss: 2.142730140686035\n",
      "Training loss: 2.1339700698852537\n",
      "Training loss: 2.1405475616455076\n",
      "Training loss: 2.131863594055176\n",
      "Training loss: 2.1383975982666015\n",
      "Training loss: 2.1297704696655275\n",
      "Training loss: 2.136247253417969\n",
      "Training loss: 2.1276790618896486\n",
      "Training loss: 2.1341176986694337\n",
      "Training loss: 2.1256038665771486\n",
      "Training loss: 2.131989097595215\n",
      "Training loss: 2.123524284362793\n",
      "Training loss: 2.1298641204833983\n",
      "Training loss: 2.1214727401733398\n",
      "Training loss: 2.1277589797973633\n",
      "Training loss: 2.1194293975830076\n",
      "Training loss: 2.1256505966186525\n",
      "Training loss: 2.1173633575439452\n",
      "Training loss: 2.123544120788574\n",
      "Training loss: 2.115324020385742\n",
      "Training loss: 2.1214614868164063\n",
      "Training loss: 2.113302230834961\n",
      "Training loss: 2.119395065307617\n",
      "Training loss: 2.111298179626465\n",
      "Training loss: 2.117344856262207\n",
      "Training loss: 2.1093204498291014\n",
      "Training loss: 2.115316390991211\n",
      "Training loss: 2.1073421478271483\n",
      "Training loss: 2.1132858276367186\n",
      "Training loss: 2.1053531646728514\n",
      "Training loss: 2.1112482070922853\n",
      "Training loss: 2.103368949890137\n",
      "Training loss: 2.1092239379882813\n",
      "Training loss: 2.1013954162597654\n",
      "Training loss: 2.1072086334228515\n",
      "Training loss: 2.0994504928588866\n",
      "Training loss: 2.1052167892456053\n",
      "Training loss: 2.0974971771240236\n",
      "Training loss: 2.103208541870117\n",
      "Training loss: 2.0955568313598634\n",
      "Training loss: 2.1012313842773436\n",
      "Training loss: 2.0936296463012694\n",
      "Training loss: 2.0992572784423826\n",
      "Training loss: 2.0917171478271483\n",
      "Training loss: 2.0972976684570312\n",
      "Training loss: 2.0898033142089845\n",
      "Training loss: 2.0953353881835937\n",
      "Training loss: 2.0878902435302735\n",
      "Training loss: 2.0933895111083984\n",
      "Training loss: 2.0860118865966797\n",
      "Training loss: 2.091472625732422\n",
      "Training loss: 2.0841421127319335\n",
      "Training loss: 2.0895509719848633\n",
      "Training loss: 2.0822704315185545\n",
      "Training loss: 2.087634468078613\n",
      "Training loss: 2.080405044555664\n",
      "Training loss: 2.085739517211914\n",
      "Training loss: 2.078556442260742\n",
      "Training loss: 2.0838462829589846\n",
      "Training loss: 2.0767335891723633\n",
      "Training loss: 2.0819759368896484\n",
      "Training loss: 2.0749011993408204\n",
      "Training loss: 2.0800939559936524\n",
      "Training loss: 2.0730737686157226\n",
      "Training loss: 2.078243446350098\n",
      "Training loss: 2.0712665557861327\n",
      "Training loss: 2.0763923645019533\n",
      "Training loss: 2.069466781616211\n",
      "Training loss: 2.0745569229125977\n",
      "Training loss: 2.067691612243652\n",
      "Training loss: 2.07274169921875\n",
      "Training loss: 2.065908432006836\n",
      "Training loss: 2.070920944213867\n",
      "Training loss: 2.064145088195801\n",
      "Training loss: 2.069108772277832\n",
      "Training loss: 2.0623693466186523\n",
      "Training loss: 2.067290687561035\n",
      "Training loss: 2.0606006622314452\n",
      "Training loss: 2.0654880523681642\n",
      "Training loss: 2.058858871459961\n",
      "Training loss: 2.0637035369873047\n",
      "Training loss: 2.057110595703125\n",
      "Training loss: 2.061923027038574\n",
      "Training loss: 2.055379104614258\n",
      "Training loss: 2.060158348083496\n",
      "Training loss: 2.0536514282226563\n",
      "Training loss: 2.0584012985229494\n",
      "Training loss: 2.0519533157348633\n",
      "Training loss: 2.0566587448120117\n",
      "Training loss: 2.0502506256103517\n",
      "Training loss: 2.0549104690551756\n",
      "Training loss: 2.0485466003417967\n",
      "Training loss: 2.053174591064453\n",
      "Training loss: 2.0468597412109375\n",
      "Training loss: 2.0514541625976563\n",
      "Training loss: 2.045171928405762\n",
      "Training loss: 2.0497283935546875\n",
      "Training loss: 2.043505096435547\n",
      "Training loss: 2.048025131225586\n",
      "Training loss: 2.0418413162231444\n",
      "Training loss: 2.046324920654297\n",
      "Training loss: 2.040187454223633\n",
      "Training loss: 2.044643783569336\n",
      "Training loss: 2.038547897338867\n",
      "Training loss: 2.042965126037598\n",
      "Training loss: 2.0369068145751954\n",
      "Training loss: 2.0412899017333985\n",
      "Training loss: 2.0352705001831053\n",
      "Training loss: 2.039613151550293\n",
      "Training loss: 2.033646011352539\n",
      "Training loss: 2.0379528045654296\n",
      "Training loss: 2.0320301055908203\n",
      "Training loss: 2.0363054275512695\n",
      "Training loss: 2.030405807495117\n",
      "Training loss: 2.034650421142578\n",
      "Training loss: 2.0287925720214846\n",
      "Training loss: 2.0330011367797853\n",
      "Training loss: 2.027189254760742\n",
      "Training loss: 2.031367874145508\n",
      "Training loss: 2.02559871673584\n",
      "Training loss: 2.029744338989258\n",
      "Training loss: 2.024019241333008\n",
      "Training loss: 2.0281301498413087\n",
      "Training loss: 2.0224435806274412\n",
      "Training loss: 2.026520919799805\n",
      "Training loss: 2.020884323120117\n",
      "Training loss: 2.0249265670776366\n",
      "Training loss: 2.019311714172363\n",
      "Training loss: 2.0233203887939455\n",
      "Training loss: 2.0177684783935548\n",
      "Training loss: 2.0217464447021483\n",
      "Training loss: 2.016218566894531\n",
      "Training loss: 2.020167922973633\n",
      "Training loss: 2.0146785736083985\n",
      "Training loss: 2.018603515625\n",
      "Training loss: 2.013161849975586\n",
      "Training loss: 2.017048645019531\n",
      "Training loss: 2.011630630493164\n",
      "Training loss: 2.015493392944336\n",
      "Training loss: 2.010121536254883\n",
      "Training loss: 2.0139570236206055\n",
      "Training loss: 2.0086200714111326\n",
      "Training loss: 2.012419891357422\n",
      "Training loss: 2.007124900817871\n",
      "Training loss: 2.0108985900878906\n",
      "Training loss: 2.005630302429199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 2.0093896865844725\n",
      "Training loss: 2.0041675567626953\n",
      "Training loss: 2.0078765869140627\n",
      "Training loss: 2.0026828765869142\n",
      "Training loss: 2.0063730239868165\n",
      "Training loss: 2.0012264251708984\n",
      "Training loss: 2.0048854827880858\n",
      "Training loss: 1.9997638702392577\n",
      "Training loss: 2.0033769607543945\n",
      "Training loss: 1.9983003616333008\n",
      "Training loss: 2.0018989562988283\n",
      "Training loss: 1.996854019165039\n",
      "Training loss: 2.0004194259643553\n",
      "Training loss: 1.9954242706298828\n",
      "Training loss: 1.9989585876464844\n",
      "Training loss: 1.9939912796020507\n",
      "Training loss: 1.9975032806396484\n",
      "Training loss: 1.9925575256347656\n",
      "Training loss: 1.9960460662841797\n",
      "Training loss: 1.9911357879638671\n",
      "Training loss: 1.9945953369140625\n",
      "Training loss: 1.9897319793701171\n",
      "Training loss: 1.9931560516357423\n",
      "Training loss: 1.988326644897461\n",
      "Training loss: 1.991727066040039\n",
      "Training loss: 1.9869272232055664\n",
      "Training loss: 1.9903003692626953\n",
      "Training loss: 1.985524559020996\n",
      "Training loss: 1.988863182067871\n",
      "Training loss: 1.9841215133666992\n",
      "Training loss: 1.987436866760254\n",
      "Training loss: 1.982741928100586\n",
      "Training loss: 1.9860437393188477\n",
      "Training loss: 1.9813793182373047\n",
      "Training loss: 1.984649658203125\n",
      "Training loss: 1.980015754699707\n",
      "Training loss: 1.9832633972167968\n",
      "Training loss: 1.978661346435547\n",
      "Training loss: 1.9818765640258789\n",
      "Training loss: 1.9772960662841796\n",
      "Training loss: 1.9804880142211914\n",
      "Training loss: 1.9759645462036133\n",
      "Training loss: 1.9791326522827148\n",
      "Training loss: 1.9746238708496093\n",
      "Training loss: 1.977768325805664\n",
      "Training loss: 1.9732969284057618\n",
      "Training loss: 1.9764101028442382\n",
      "Training loss: 1.9719589233398438\n",
      "Training loss: 1.9750486373901368\n",
      "Training loss: 1.970637893676758\n",
      "Training loss: 1.9736972808837892\n",
      "Training loss: 1.969308090209961\n",
      "Training loss: 1.9723520278930664\n",
      "Training loss: 1.9680047988891602\n",
      "Training loss: 1.9710243225097657\n",
      "Training loss: 1.9666988372802734\n",
      "Training loss: 1.9696920394897461\n",
      "Training loss: 1.9654043197631836\n",
      "Training loss: 1.9683813095092773\n",
      "Training loss: 1.9641231536865233\n",
      "Training loss: 1.9670633316040038\n",
      "Training loss: 1.9628265380859375\n",
      "Training loss: 1.9657526016235352\n",
      "Training loss: 1.9615650177001953\n",
      "Training loss: 1.9644622802734375\n",
      "Training loss: 1.9603038787841798\n",
      "Training loss: 1.9631792068481446\n",
      "Training loss: 1.9590450286865235\n",
      "Training loss: 1.9619028091430664\n",
      "Training loss: 1.9577991485595703\n",
      "Training loss: 1.9606287002563476\n",
      "Training loss: 1.956545639038086\n",
      "Training loss: 1.9593521118164063\n",
      "Training loss: 1.955303955078125\n",
      "Training loss: 1.9580890655517578\n",
      "Training loss: 1.9540718078613282\n",
      "Training loss: 1.9568349838256835\n",
      "Training loss: 1.952840805053711\n",
      "Training loss: 1.9555831909179688\n",
      "Training loss: 1.9516262054443358\n",
      "Training loss: 1.954355239868164\n",
      "Training loss: 1.950418472290039\n",
      "Training loss: 1.95312442779541\n",
      "Training loss: 1.9492023468017579\n",
      "Training loss: 1.95189208984375\n",
      "Training loss: 1.9480100631713868\n",
      "Training loss: 1.9506681442260743\n",
      "Training loss: 1.9468128204345703\n",
      "Training loss: 1.9494476318359375\n",
      "Training loss: 1.9456178665161132\n",
      "Training loss: 1.948233413696289\n",
      "Training loss: 1.9444259643554687\n",
      "Training loss: 1.9470195770263672\n",
      "Training loss: 1.9432435989379884\n",
      "Training loss: 1.945807647705078\n",
      "Training loss: 1.9420484542846679\n",
      "Training loss: 1.9445947647094726\n",
      "Training loss: 1.9408777236938477\n",
      "Training loss: 1.943398666381836\n",
      "Training loss: 1.9396968841552735\n",
      "Training loss: 1.9422187805175781\n",
      "Training loss: 1.9385486602783204\n",
      "Training loss: 1.9410413742065429\n",
      "Training loss: 1.9374027252197266\n",
      "Training loss: 1.939887237548828\n",
      "Training loss: 1.936263656616211\n",
      "Training loss: 1.9387226104736328\n",
      "Training loss: 1.9351261138916016\n",
      "Training loss: 1.9375560760498047\n",
      "Training loss: 1.9339813232421874\n",
      "Training loss: 1.9363874435424804\n",
      "Training loss: 1.9328437805175782\n",
      "Training loss: 1.935237503051758\n",
      "Training loss: 1.931707763671875\n",
      "Training loss: 1.9340839385986328\n",
      "Training loss: 1.930582046508789\n",
      "Training loss: 1.9329395294189453\n",
      "Training loss: 1.9294692993164062\n",
      "Training loss: 1.9318004608154298\n",
      "Training loss: 1.9283557891845704\n",
      "Training loss: 1.930672836303711\n",
      "Training loss: 1.927243423461914\n",
      "Training loss: 1.9295364379882813\n",
      "Training loss: 1.926138687133789\n",
      "Training loss: 1.9284215927124024\n",
      "Training loss: 1.9250360488891602\n",
      "Training loss: 1.9272993087768555\n",
      "Training loss: 1.9239360809326171\n",
      "Training loss: 1.9261856079101562\n",
      "Training loss: 1.9228530883789063\n",
      "Training loss: 1.9250782012939454\n",
      "Training loss: 1.9217779159545898\n",
      "Training loss: 1.9239805221557618\n",
      "Training loss: 1.9207033157348632\n",
      "Training loss: 1.9229032516479492\n",
      "Training loss: 1.9196426391601562\n",
      "Training loss: 1.9218143463134765\n",
      "Training loss: 1.918581771850586\n",
      "Training loss: 1.9207504272460938\n",
      "Training loss: 1.9175405502319336\n",
      "Training loss: 1.9196849822998048\n",
      "Training loss: 1.9164974212646484\n",
      "Training loss: 1.9186182022094727\n",
      "Training loss: 1.915456199645996\n",
      "Training loss: 1.917551612854004\n",
      "Training loss: 1.914413833618164\n",
      "Training loss: 1.9164907455444335\n",
      "Training loss: 1.9133655548095703\n",
      "Training loss: 1.9154329299926758\n",
      "Training loss: 1.9123374938964843\n",
      "Training loss: 1.9143884658813477\n",
      "Training loss: 1.9113079071044923\n",
      "Training loss: 1.9133495330810546\n",
      "Training loss: 1.9102886199951172\n",
      "Training loss: 1.912325668334961\n",
      "Training loss: 1.9092716217041015\n",
      "Training loss: 1.9112823486328125\n",
      "Training loss: 1.9082561492919923\n",
      "Training loss: 1.9102413177490234\n",
      "Training loss: 1.907242202758789\n",
      "Training loss: 1.9092082977294922\n",
      "Training loss: 1.9062358856201171\n",
      "Training loss: 1.9081893920898438\n",
      "Training loss: 1.905248260498047\n",
      "Training loss: 1.9071868896484374\n",
      "Training loss: 1.9042499542236329\n",
      "Training loss: 1.906179428100586\n",
      "Training loss: 1.903261947631836\n",
      "Training loss: 1.905172348022461\n",
      "Training loss: 1.9022783279418944\n",
      "Training loss: 1.904179000854492\n",
      "Training loss: 1.901310348510742\n",
      "Training loss: 1.903181266784668\n",
      "Training loss: 1.9003204345703124\n",
      "Training loss: 1.902186393737793\n",
      "Training loss: 1.8993492126464844\n",
      "Training loss: 1.9012039184570313\n",
      "Training loss: 1.8983936309814453\n",
      "Training loss: 1.900225830078125\n",
      "Training loss: 1.8974363327026367\n",
      "Training loss: 1.8992601394653321\n",
      "Training loss: 1.8964754104614259\n",
      "Training loss: 1.898291015625\n",
      "Training loss: 1.8955427169799806\n",
      "Training loss: 1.8973373413085937\n",
      "Training loss: 1.894603157043457\n",
      "Training loss: 1.896376609802246\n",
      "Training loss: 1.8936506271362306\n",
      "Training loss: 1.8954158782958985\n",
      "Training loss: 1.8927162170410157\n",
      "Training loss: 1.8944766998291016\n",
      "Training loss: 1.8917961120605469\n",
      "Training loss: 1.8935234069824218\n",
      "Training loss: 1.890864944458008\n",
      "Training loss: 1.8925861358642577\n",
      "Training loss: 1.8899425506591796\n",
      "Training loss: 1.8916553497314452\n",
      "Training loss: 1.8890346527099608\n",
      "Training loss: 1.8907329559326171\n",
      "Training loss: 1.888132095336914\n",
      "Training loss: 1.8898117065429687\n",
      "Training loss: 1.8872243881225585\n",
      "Training loss: 1.8888839721679687\n",
      "Training loss: 1.8863245010375977\n",
      "Training loss: 1.8879802703857422\n",
      "Training loss: 1.885432815551758\n",
      "Training loss: 1.8870792388916016\n",
      "Training loss: 1.8845447540283202\n",
      "Training loss: 1.88616886138916\n",
      "Training loss: 1.88365478515625\n",
      "Training loss: 1.8852619171142577\n",
      "Training loss: 1.8827672958374024\n",
      "Training loss: 1.8843648910522461\n",
      "Training loss: 1.8818836212158203\n",
      "Training loss: 1.883464241027832\n",
      "Training loss: 1.8810108184814454\n",
      "Training loss: 1.8825820922851562\n",
      "Training loss: 1.8801319122314453\n",
      "Training loss: 1.881689453125\n",
      "Training loss: 1.8792627334594727\n",
      "Training loss: 1.8808111190795898\n",
      "Training loss: 1.8783893585205078\n",
      "Training loss: 1.879928207397461\n",
      "Training loss: 1.8775274276733398\n",
      "Training loss: 1.8790569305419922\n",
      "Training loss: 1.876674270629883\n",
      "Training loss: 1.8781875610351562\n",
      "Training loss: 1.875813865661621\n",
      "Training loss: 1.8773138046264648\n",
      "Training loss: 1.874971580505371\n",
      "Training loss: 1.8764581680297852\n",
      "Training loss: 1.8741287231445312\n",
      "Training loss: 1.8756057739257812\n",
      "Training loss: 1.8732917785644532\n",
      "Training loss: 1.874751663208008\n",
      "Training loss: 1.8724615097045898\n",
      "Training loss: 1.8739107131958008\n",
      "Training loss: 1.871638870239258\n",
      "Training loss: 1.873074722290039\n",
      "Training loss: 1.87081298828125\n",
      "Training loss: 1.8722343444824219\n",
      "Training loss: 1.869990348815918\n",
      "Training loss: 1.8713993072509765\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 1.869169044494629\n",
      "Training loss: 1.870570182800293\n",
      "Training loss: 1.8683446884155273\n",
      "Training loss: 1.8697446823120116\n",
      "Training loss: 1.8675329208374023\n",
      "Training loss: 1.8689163208007813\n",
      "Training loss: 1.8667139053344726\n",
      "Training loss: 1.8680919647216796\n",
      "Training loss: 1.865920639038086\n",
      "Training loss: 1.867276954650879\n",
      "Training loss: 1.8651119232177735\n",
      "Training loss: 1.8664661407470704\n",
      "Training loss: 1.864328384399414\n",
      "Training loss: 1.8656675338745117\n",
      "Training loss: 1.8635412216186524\n",
      "Training loss: 1.8648710250854492\n",
      "Training loss: 1.862764549255371\n",
      "Training loss: 1.86407527923584\n",
      "Training loss: 1.8619768142700195\n",
      "Training loss: 1.86328125\n",
      "Training loss: 1.8611896514892579\n",
      "Training loss: 1.862483024597168\n",
      "Training loss: 1.8604047775268555\n",
      "Training loss: 1.8616880416870116\n",
      "Training loss: 1.859636116027832\n",
      "Training loss: 1.8609121322631836\n",
      "Training loss: 1.8588645935058594\n",
      "Training loss: 1.8601251602172852\n",
      "Training loss: 1.8581045150756836\n",
      "Training loss: 1.8593660354614259\n",
      "Training loss: 1.8573511123657227\n",
      "Training loss: 1.8585933685302733\n",
      "Training loss: 1.8565959930419922\n",
      "Training loss: 1.8578357696533203\n",
      "Training loss: 1.8558582305908202\n",
      "Training loss: 1.8570804595947266\n",
      "Training loss: 1.8551170349121093\n",
      "Training loss: 1.8563360214233398\n",
      "Training loss: 1.854379653930664\n",
      "Training loss: 1.8555896759033204\n",
      "Training loss: 1.8536413192749024\n",
      "Training loss: 1.8548418045043946\n",
      "Training loss: 1.8529117584228516\n",
      "Training loss: 1.8540979385375977\n",
      "Training loss: 1.852180290222168\n",
      "Training loss: 1.8533472061157226\n",
      "Training loss: 1.8514471054077148\n",
      "Training loss: 1.8526132583618165\n",
      "Training loss: 1.8507205963134765\n",
      "Training loss: 1.8518829345703125\n",
      "Training loss: 1.8500057220458985\n",
      "Training loss: 1.851142120361328\n",
      "Training loss: 1.8492830276489258\n",
      "Training loss: 1.8504236221313477\n",
      "Training loss: 1.8485651016235352\n",
      "Training loss: 1.8496953964233398\n",
      "Training loss: 1.8478569030761718\n",
      "Training loss: 1.8489715576171875\n",
      "Training loss: 1.8471412658691406\n",
      "Training loss: 1.8482532501220703\n",
      "Training loss: 1.8464338302612304\n",
      "Training loss: 1.8475286483764648\n",
      "Training loss: 1.8457183837890625\n",
      "Training loss: 1.8468149185180665\n",
      "Training loss: 1.8450130462646483\n",
      "Training loss: 1.846106719970703\n",
      "Training loss: 1.844319725036621\n",
      "Training loss: 1.845393180847168\n",
      "Training loss: 1.8436243057250976\n",
      "Training loss: 1.8446855545043945\n",
      "Training loss: 1.8429332733154298\n",
      "Training loss: 1.843986701965332\n",
      "Training loss: 1.8422344207763672\n",
      "Training loss: 1.8432846069335938\n",
      "Training loss: 1.8415502548217773\n",
      "Training loss: 1.8425836563110352\n",
      "Training loss: 1.8408580780029298\n",
      "Training loss: 1.8418956756591798\n",
      "Training loss: 1.840193748474121\n",
      "Training loss: 1.8412210464477539\n",
      "Training loss: 1.8395244598388671\n",
      "Training loss: 1.8405452728271485\n",
      "Training loss: 1.838859748840332\n",
      "Training loss: 1.8398677825927734\n",
      "Training loss: 1.838203239440918\n",
      "Training loss: 1.839194107055664\n",
      "Training loss: 1.8375373840332032\n",
      "Training loss: 1.8385295867919922\n",
      "Training loss: 1.8368778228759766\n",
      "Training loss: 1.8378591537475586\n",
      "Training loss: 1.8362173080444335\n",
      "Training loss: 1.8371969223022462\n",
      "Training loss: 1.8355690002441407\n",
      "Training loss: 1.8365324020385743\n",
      "Training loss: 1.8349237442016602\n",
      "Training loss: 1.8358810424804688\n",
      "Training loss: 1.8342802047729492\n",
      "Training loss: 1.8352363586425782\n",
      "Training loss: 1.8336463928222657\n",
      "Training loss: 1.8345870971679688\n",
      "Training loss: 1.8330093383789063\n",
      "Training loss: 1.833948516845703\n",
      "Training loss: 1.8323850631713867\n",
      "Training loss: 1.8333215713500977\n",
      "Training loss: 1.8317558288574218\n",
      "Training loss: 1.8326728820800782\n",
      "Training loss: 1.8311332702636718\n",
      "Training loss: 1.8320446014404297\n",
      "Training loss: 1.8305034637451172\n",
      "Training loss: 1.8314044952392579\n",
      "Training loss: 1.8298768997192383\n",
      "Training loss: 1.8307714462280273\n",
      "Training loss: 1.8292530059814454\n",
      "Training loss: 1.8301372528076172\n",
      "Training loss: 1.8286365509033202\n",
      "Training loss: 1.8295141220092774\n",
      "Training loss: 1.8280208587646485\n",
      "Training loss: 1.8288946151733398\n",
      "Training loss: 1.8274051666259765\n",
      "Training loss: 1.8282690048217773\n",
      "Training loss: 1.8267959594726562\n",
      "Training loss: 1.8276567459106445\n",
      "Training loss: 1.826181411743164\n",
      "Training loss: 1.8270389556884765\n",
      "Training loss: 1.8255840301513673\n",
      "Training loss: 1.826429557800293\n",
      "Training loss: 1.8249898910522462\n",
      "Training loss: 1.8258197784423829\n",
      "Training loss: 1.8243907928466796\n",
      "Training loss: 1.8252223968505858\n",
      "Training loss: 1.8237966537475585\n",
      "Training loss: 1.8246288299560547\n",
      "Training loss: 1.823212242126465\n",
      "Training loss: 1.8240262985229492\n",
      "Training loss: 1.8226234436035156\n",
      "Training loss: 1.8234382629394532\n",
      "Training loss: 1.822043228149414\n",
      "Training loss: 1.8228527069091798\n",
      "Training loss: 1.8214664459228516\n",
      "Training loss: 1.8222675323486328\n",
      "Training loss: 1.8208889007568358\n",
      "Training loss: 1.8216787338256837\n",
      "Training loss: 1.820316696166992\n",
      "Training loss: 1.8210962295532227\n",
      "Training loss: 1.8197399139404298\n",
      "Training loss: 1.8205158233642578\n",
      "Training loss: 1.8191682815551757\n",
      "Training loss: 1.8199390411376952\n",
      "Training loss: 1.8186063766479492\n",
      "Training loss: 1.8193689346313477\n",
      "Training loss: 1.8180341720581055\n",
      "Training loss: 1.818795394897461\n",
      "Training loss: 1.8174795150756835\n",
      "Training loss: 1.818226432800293\n",
      "Training loss: 1.8169200897216797\n",
      "Training loss: 1.817662239074707\n",
      "Training loss: 1.8163604736328125\n",
      "Training loss: 1.8171016693115234\n",
      "Training loss: 1.8158071517944336\n",
      "Training loss: 1.8165468215942382\n",
      "Training loss: 1.8152585983276368\n",
      "Training loss: 1.8159845352172852\n",
      "Training loss: 1.8147098541259765\n",
      "Training loss: 1.8154306411743164\n",
      "Training loss: 1.8141677856445313\n",
      "Training loss: 1.8148868560791016\n",
      "Training loss: 1.8136272430419922\n",
      "Training loss: 1.8143400192260741\n",
      "Training loss: 1.813084030151367\n",
      "Training loss: 1.813784408569336\n",
      "Training loss: 1.812539291381836\n",
      "Training loss: 1.8132284164428711\n",
      "Training loss: 1.8119953155517579\n",
      "Training loss: 1.812687301635742\n",
      "Training loss: 1.811464500427246\n",
      "Training loss: 1.8121471405029297\n",
      "Training loss: 1.8109344482421874\n",
      "Training loss: 1.8116077423095702\n",
      "Training loss: 1.810396957397461\n",
      "Training loss: 1.811067771911621\n",
      "Training loss: 1.8098705291748047\n",
      "Training loss: 1.8105401992797852\n",
      "Training loss: 1.809343147277832\n",
      "Training loss: 1.810001754760742\n",
      "Training loss: 1.8088232040405274\n",
      "Training loss: 1.8094715118408202\n",
      "Training loss: 1.8082916259765625\n",
      "Training loss: 1.8089426040649415\n",
      "Training loss: 1.8077747344970703\n",
      "Training loss: 1.8084278106689453\n",
      "Training loss: 1.8072607040405273\n",
      "Training loss: 1.8079057693481446\n",
      "Training loss: 1.8067415237426758\n",
      "Training loss: 1.807388687133789\n",
      "Training loss: 1.8062458038330078\n",
      "Training loss: 1.806877899169922\n",
      "Training loss: 1.8057472229003906\n",
      "Training loss: 1.8063776016235351\n",
      "Training loss: 1.8052492141723633\n",
      "Training loss: 1.8058765411376954\n",
      "Training loss: 1.8047460556030273\n",
      "Training loss: 1.8053726196289062\n",
      "Training loss: 1.80426082611084\n",
      "Training loss: 1.8048805236816405\n",
      "Training loss: 1.80377254486084\n",
      "Training loss: 1.8043701171875\n",
      "Training loss: 1.8032649993896483\n",
      "Training loss: 1.8038654327392578\n",
      "Training loss: 1.8027704238891602\n",
      "Training loss: 1.8033601760864257\n",
      "Training loss: 1.8022777557373046\n",
      "Training loss: 1.802862548828125\n",
      "Training loss: 1.8017848968505858\n",
      "Training loss: 1.8023696899414063\n",
      "Training loss: 1.8013044357299806\n",
      "Training loss: 1.801881980895996\n",
      "Training loss: 1.8008182525634766\n",
      "Training loss: 1.8013944625854492\n",
      "Training loss: 1.8003440856933595\n",
      "Training loss: 1.800912094116211\n",
      "Training loss: 1.7998653411865235\n",
      "Training loss: 1.8004343032836914\n",
      "Training loss: 1.7993925094604493\n",
      "Training loss: 1.7999443054199218\n",
      "Training loss: 1.7989233016967774\n",
      "Training loss: 1.7994815826416015\n",
      "Training loss: 1.7984607696533204\n",
      "Training loss: 1.7990045547485352\n",
      "Training loss: 1.797995376586914\n",
      "Training loss: 1.7985380172729493\n",
      "Training loss: 1.797543716430664\n",
      "Training loss: 1.798080062866211\n",
      "Training loss: 1.797083282470703\n",
      "Training loss: 1.7976186752319336\n",
      "Training loss: 1.7966289520263672\n",
      "Training loss: 1.797161865234375\n",
      "Training loss: 1.7961767196655274\n",
      "Training loss: 1.7967050552368165\n",
      "Training loss: 1.7957252502441405\n",
      "Training loss: 1.7962507247924804\n",
      "Training loss: 1.7952747344970703\n",
      "Training loss: 1.7957918167114257\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 1.7948144912719726\n",
      "Training loss: 1.7953340530395507\n",
      "Training loss: 1.7943729400634765\n",
      "Training loss: 1.7948822021484374\n",
      "Training loss: 1.7939281463623047\n",
      "Training loss: 1.794427490234375\n",
      "Training loss: 1.7934810638427734\n",
      "Training loss: 1.7939807891845703\n",
      "Training loss: 1.793039894104004\n",
      "Training loss: 1.793531608581543\n",
      "Training loss: 1.7925968170166016\n",
      "Training loss: 1.7930917739868164\n",
      "Training loss: 1.7921585083007812\n",
      "Training loss: 1.7926450729370118\n",
      "Training loss: 1.7917240142822266\n",
      "Training loss: 1.7922039031982422\n",
      "Training loss: 1.7912837982177734\n",
      "Training loss: 1.7917713165283202\n",
      "Training loss: 1.7908563613891602\n",
      "Training loss: 1.791326141357422\n",
      "Training loss: 1.7904180526733398\n",
      "Training loss: 1.790883445739746\n",
      "Training loss: 1.7899837493896484\n",
      "Training loss: 1.790456199645996\n",
      "Training loss: 1.789571189880371\n",
      "Training loss: 1.7900426864624024\n",
      "Training loss: 1.7891580581665039\n",
      "Training loss: 1.789619255065918\n",
      "Training loss: 1.7887399673461915\n",
      "Training loss: 1.7892038345336914\n",
      "Training loss: 1.7883255004882812\n",
      "Training loss: 1.7887775421142578\n",
      "Training loss: 1.787904930114746\n",
      "Training loss: 1.7883565902709961\n",
      "Training loss: 1.7874874114990233\n",
      "Training loss: 1.7879436492919922\n",
      "Training loss: 1.787087631225586\n",
      "Training loss: 1.787526512145996\n",
      "Training loss: 1.7866771697998047\n",
      "Training loss: 1.7871143341064453\n",
      "Training loss: 1.786271858215332\n",
      "Training loss: 1.7867067337036133\n",
      "Training loss: 1.7858633041381835\n",
      "Training loss: 1.7862958908081055\n",
      "Training loss: 1.7854629516601563\n",
      "Training loss: 1.7858921051025392\n",
      "Training loss: 1.78505859375\n",
      "Training loss: 1.7854846954345702\n",
      "Training loss: 1.7846622467041016\n",
      "Training loss: 1.7850908279418944\n",
      "Training loss: 1.7842681884765625\n",
      "Training loss: 1.7846824645996093\n",
      "Training loss: 1.7838762283325196\n",
      "Training loss: 1.7842864990234375\n",
      "Training loss: 1.783473587036133\n",
      "Training loss: 1.7838829040527344\n",
      "Training loss: 1.7830772399902344\n",
      "Training loss: 1.7834827423095703\n",
      "Training loss: 1.7826925277709962\n",
      "Training loss: 1.783085250854492\n",
      "Training loss: 1.7822948455810548\n",
      "Training loss: 1.7826889038085938\n",
      "Training loss: 1.7819067001342774\n",
      "Training loss: 1.782303810119629\n",
      "Training loss: 1.781517219543457\n",
      "Training loss: 1.781914520263672\n",
      "Training loss: 1.781138038635254\n",
      "Training loss: 1.7815235137939454\n",
      "Training loss: 1.7807550430297852\n",
      "Training loss: 1.781136703491211\n",
      "Training loss: 1.7803779602050782\n",
      "Training loss: 1.780757522583008\n",
      "Training loss: 1.7800045013427734\n",
      "Training loss: 1.780379867553711\n",
      "Training loss: 1.7796222686767578\n",
      "Training loss: 1.7799932479858398\n",
      "Training loss: 1.7792482376098633\n",
      "Training loss: 1.7796211242675781\n",
      "Training loss: 1.7788707733154296\n",
      "Training loss: 1.779240608215332\n",
      "Training loss: 1.7785009384155273\n",
      "Training loss: 1.778867530822754\n",
      "Training loss: 1.77813720703125\n",
      "Training loss: 1.7785011291503907\n",
      "Training loss: 1.777766799926758\n",
      "Training loss: 1.7781240463256835\n",
      "Training loss: 1.777400588989258\n",
      "Training loss: 1.7777591705322267\n",
      "Training loss: 1.7770399093627929\n",
      "Training loss: 1.777397918701172\n",
      "Training loss: 1.7766868591308593\n",
      "Training loss: 1.777029800415039\n",
      "Training loss: 1.7763153076171876\n",
      "Training loss: 1.776662254333496\n",
      "Training loss: 1.7759639739990234\n",
      "Training loss: 1.776303482055664\n",
      "Training loss: 1.775609016418457\n",
      "Training loss: 1.7759527206420898\n",
      "Training loss: 1.7752628326416016\n",
      "Training loss: 1.7755935668945313\n",
      "Training loss: 1.774907684326172\n",
      "Training loss: 1.7752317428588866\n",
      "Training loss: 1.7745464324951172\n",
      "Training loss: 1.7748828887939454\n",
      "Training loss: 1.7742042541503906\n",
      "Training loss: 1.7745309829711915\n",
      "Training loss: 1.7738557815551759\n",
      "Training loss: 1.7741798400878905\n",
      "Training loss: 1.7735061645507812\n",
      "Training loss: 1.7738285064697266\n",
      "Training loss: 1.7731639862060546\n",
      "Training loss: 1.7734752655029298\n",
      "Training loss: 1.7728076934814454\n",
      "Training loss: 1.773128128051758\n",
      "Training loss: 1.7724720001220704\n",
      "Training loss: 1.772787094116211\n",
      "Training loss: 1.772130584716797\n",
      "Training loss: 1.7724407196044922\n",
      "Training loss: 1.7717967987060548\n",
      "Training loss: 1.7721054077148437\n",
      "Training loss: 1.7714599609375\n",
      "Training loss: 1.771766471862793\n",
      "Training loss: 1.7711231231689453\n",
      "Training loss: 1.7714317321777344\n",
      "Training loss: 1.770796775817871\n",
      "Training loss: 1.7711002349853515\n",
      "Training loss: 1.7704706192016602\n",
      "Training loss: 1.7707704544067382\n",
      "Training loss: 1.7701366424560547\n",
      "Training loss: 1.7704343795776367\n",
      "Training loss: 1.7698116302490234\n",
      "Training loss: 1.770108985900879\n",
      "Training loss: 1.7694839477539062\n",
      "Training loss: 1.769781494140625\n",
      "Training loss: 1.7691532135009767\n",
      "Training loss: 1.769448471069336\n",
      "Training loss: 1.768838119506836\n",
      "Training loss: 1.7691299438476562\n",
      "Training loss: 1.7685169219970702\n",
      "Training loss: 1.7687992095947265\n",
      "Training loss: 1.7681964874267577\n",
      "Training loss: 1.7684761047363282\n",
      "Training loss: 1.7678768157958984\n",
      "Training loss: 1.7681591033935546\n",
      "Training loss: 1.7675622940063476\n",
      "Training loss: 1.7678445816040038\n",
      "Training loss: 1.7672500610351562\n",
      "Training loss: 1.7675228118896484\n",
      "Training loss: 1.766934585571289\n",
      "Training loss: 1.767207717895508\n",
      "Training loss: 1.7666311264038086\n",
      "Training loss: 1.7668994903564452\n",
      "Training loss: 1.7663223266601562\n",
      "Training loss: 1.7665912628173828\n",
      "Training loss: 1.766012191772461\n",
      "Training loss: 1.7662731170654298\n",
      "Training loss: 1.7657073974609374\n",
      "Training loss: 1.7659643173217774\n",
      "Training loss: 1.765397834777832\n",
      "Training loss: 1.765660285949707\n",
      "Training loss: 1.7650928497314453\n",
      "Training loss: 1.7653501510620118\n",
      "Training loss: 1.7647872924804688\n",
      "Training loss: 1.7650423049926758\n",
      "Training loss: 1.7644844055175781\n",
      "Training loss: 1.764737892150879\n",
      "Training loss: 1.764181137084961\n",
      "Training loss: 1.764437484741211\n",
      "Training loss: 1.763881301879883\n",
      "Training loss: 1.7641267776489258\n",
      "Training loss: 1.763577651977539\n",
      "Training loss: 1.7638246536254882\n",
      "Training loss: 1.76328125\n",
      "Training loss: 1.7635307312011719\n",
      "Training loss: 1.7629947662353516\n",
      "Training loss: 1.7632272720336915\n",
      "Training loss: 1.7626914978027344\n",
      "Training loss: 1.762934684753418\n",
      "Training loss: 1.7623991012573241\n",
      "Training loss: 1.762640380859375\n",
      "Training loss: 1.762109375\n",
      "Training loss: 1.7623462677001953\n",
      "Training loss: 1.7618202209472655\n",
      "Training loss: 1.762051773071289\n",
      "Training loss: 1.7615320205688476\n",
      "Training loss: 1.7617704391479492\n",
      "Training loss: 1.7612443923950196\n",
      "Training loss: 1.761477279663086\n",
      "Training loss: 1.7609624862670898\n",
      "Training loss: 1.7611831665039062\n",
      "Training loss: 1.7606851577758789\n",
      "Training loss: 1.760909652709961\n",
      "Training loss: 1.7603961944580078\n",
      "Training loss: 1.7606241226196289\n",
      "Training loss: 1.7601213455200195\n",
      "Training loss: 1.7603408813476562\n",
      "Training loss: 1.7598459243774414\n",
      "Training loss: 1.760055923461914\n",
      "Training loss: 1.7595695495605468\n",
      "Training loss: 1.7597867965698242\n",
      "Training loss: 1.7592958450317382\n",
      "Training loss: 1.7595109939575195\n",
      "Training loss: 1.7590299606323243\n",
      "Training loss: 1.759238052368164\n",
      "Training loss: 1.758763885498047\n",
      "Training loss: 1.7589702606201172\n",
      "Training loss: 1.7584873199462892\n",
      "Training loss: 1.7587059020996094\n",
      "Training loss: 1.7582252502441407\n",
      "Training loss: 1.758432388305664\n",
      "Training loss: 1.7579593658447266\n",
      "Training loss: 1.7581663131713867\n",
      "Training loss: 1.757695198059082\n",
      "Training loss: 1.7578994750976562\n",
      "Training loss: 1.7574308395385743\n",
      "Training loss: 1.7576370239257812\n",
      "Training loss: 1.7571697235107422\n",
      "Training loss: 1.757369041442871\n",
      "Training loss: 1.7569103240966797\n",
      "Training loss: 1.7571098327636718\n",
      "Training loss: 1.7566516876220704\n",
      "Training loss: 1.756850242614746\n",
      "Training loss: 1.7563907623291015\n",
      "Training loss: 1.7565868377685547\n",
      "Training loss: 1.756129837036133\n",
      "Training loss: 1.7563274383544922\n",
      "Training loss: 1.7558801651000977\n",
      "Training loss: 1.7560684204101562\n",
      "Training loss: 1.7556175231933593\n",
      "Training loss: 1.7558076858520508\n",
      "Training loss: 1.7553627014160156\n",
      "Training loss: 1.755544662475586\n",
      "Training loss: 1.7551128387451171\n",
      "Training loss: 1.7552921295166015\n",
      "Training loss: 1.7548559188842774\n",
      "Training loss: 1.7550403594970703\n",
      "Training loss: 1.7546066284179687\n",
      "Training loss: 1.754785919189453\n",
      "Training loss: 1.7543495178222657\n",
      "Training loss: 1.754532241821289\n",
      "Training loss: 1.7541004180908204\n",
      "Training loss: 1.7542821884155273\n",
      "Training loss: 1.7538469314575196\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 1.7540264129638672\n",
      "Training loss: 1.7536006927490235\n",
      "Training loss: 1.7537691116333007\n",
      "Training loss: 1.7533435821533203\n",
      "Training loss: 1.7535263061523438\n",
      "Training loss: 1.753101921081543\n",
      "Training loss: 1.7532672882080078\n",
      "Training loss: 1.7528568267822267\n",
      "Training loss: 1.7530246734619142\n",
      "Training loss: 1.7526159286499023\n",
      "Training loss: 1.7527769088745118\n",
      "Training loss: 1.7523700714111328\n",
      "Training loss: 1.7525394439697266\n",
      "Training loss: 1.7521345138549804\n",
      "Training loss: 1.7522968292236327\n",
      "Training loss: 1.751889419555664\n",
      "Training loss: 1.7520574569702148\n",
      "Training loss: 1.7516557693481445\n",
      "Training loss: 1.751816177368164\n",
      "Training loss: 1.751418685913086\n",
      "Training loss: 1.7515785217285156\n",
      "Training loss: 1.7511819839477538\n",
      "Training loss: 1.7513389587402344\n",
      "Training loss: 1.7509456634521485\n",
      "Training loss: 1.7510971069335937\n",
      "Training loss: 1.7507085800170898\n",
      "Training loss: 1.750864028930664\n",
      "Training loss: 1.750474739074707\n",
      "Training loss: 1.750629997253418\n",
      "Training loss: 1.7502408981323243\n",
      "Training loss: 1.7503904342651366\n",
      "Training loss: 1.7500125885009765\n",
      "Training loss: 1.750166130065918\n",
      "Training loss: 1.7497846603393554\n",
      "Training loss: 1.7499406814575196\n",
      "Training loss: 1.749556541442871\n",
      "Training loss: 1.7497163772583009\n",
      "Training loss: 1.749339485168457\n",
      "Training loss: 1.7494956970214843\n",
      "Training loss: 1.7491212844848634\n",
      "Training loss: 1.7492813110351562\n",
      "Training loss: 1.7489025115966796\n",
      "Training loss: 1.7490480422973633\n",
      "Training loss: 1.7486797332763673\n",
      "Training loss: 1.748819923400879\n",
      "Training loss: 1.7484594345092774\n",
      "Training loss: 1.7485996246337892\n",
      "Training loss: 1.748239517211914\n",
      "Training loss: 1.748387336730957\n",
      "Training loss: 1.7480253219604491\n",
      "Training loss: 1.7481725692749024\n",
      "Training loss: 1.7478109359741212\n",
      "Training loss: 1.747954750061035\n",
      "Training loss: 1.7476024627685547\n",
      "Training loss: 1.7477420806884765\n",
      "Training loss: 1.7473834991455077\n",
      "Training loss: 1.7475269317626954\n",
      "Training loss: 1.7471729278564454\n",
      "Training loss: 1.7473081588745116\n",
      "Training loss: 1.7469598770141601\n",
      "Training loss: 1.7470922470092773\n",
      "Training loss: 1.7467439651489258\n",
      "Training loss: 1.7468786239624023\n",
      "Training loss: 1.7465356826782226\n",
      "Training loss: 1.7466716766357422\n",
      "Training loss: 1.7463268280029296\n",
      "Training loss: 1.746464157104492\n",
      "Training loss: 1.7461198806762694\n",
      "Training loss: 1.746246910095215\n",
      "Training loss: 1.7459093093872071\n",
      "Training loss: 1.7460430145263672\n",
      "Training loss: 1.7457054138183594\n",
      "Training loss: 1.7458324432373047\n",
      "Training loss: 1.7455011367797852\n",
      "Training loss: 1.7456195831298829\n",
      "Training loss: 1.7452848434448243\n",
      "Training loss: 1.7454082489013671\n",
      "Training loss: 1.7450780868530273\n",
      "Training loss: 1.7452001571655273\n",
      "Training loss: 1.744875144958496\n",
      "Training loss: 1.744990348815918\n",
      "Training loss: 1.744668960571289\n",
      "Training loss: 1.7447885513305663\n",
      "Training loss: 1.7444656372070313\n",
      "Training loss: 1.7445842742919921\n",
      "Training loss: 1.7442661285400392\n",
      "Training loss: 1.744382667541504\n",
      "Training loss: 1.744065284729004\n",
      "Training loss: 1.7441843032836915\n",
      "Training loss: 1.7438682556152343\n",
      "Training loss: 1.7439905166625977\n",
      "Training loss: 1.743674087524414\n",
      "Training loss: 1.7437889099121093\n",
      "Training loss: 1.7434700012207032\n",
      "Training loss: 1.743584442138672\n",
      "Training loss: 1.7432723999023438\n",
      "Training loss: 1.7433881759643555\n",
      "Training loss: 1.7430816650390626\n",
      "Training loss: 1.7431955337524414\n",
      "Training loss: 1.7428850173950194\n",
      "Training loss: 1.7430017471313477\n",
      "Training loss: 1.7426937103271485\n",
      "Training loss: 1.742807388305664\n",
      "Training loss: 1.742498779296875\n",
      "Training loss: 1.742609405517578\n",
      "Training loss: 1.7423130035400392\n",
      "Training loss: 1.7424198150634767\n",
      "Training loss: 1.7421186447143555\n",
      "Training loss: 1.7422321319580079\n",
      "Training loss: 1.741933822631836\n",
      "Training loss: 1.7420425415039062\n",
      "Training loss: 1.7417491912841796\n",
      "Training loss: 1.7418577194213867\n",
      "Training loss: 1.7415668487548828\n",
      "Training loss: 1.7416656494140625\n",
      "Training loss: 1.7413738250732422\n",
      "Training loss: 1.741476821899414\n",
      "Training loss: 1.7411897659301758\n",
      "Training loss: 1.7412935256958009\n",
      "Training loss: 1.7409996032714843\n",
      "Training loss: 1.7411005020141601\n",
      "Training loss: 1.7408172607421875\n",
      "Training loss: 1.7409135818481445\n",
      "Training loss: 1.7406299591064454\n",
      "Training loss: 1.740725326538086\n",
      "Training loss: 1.7404487609863282\n",
      "Training loss: 1.7405467987060548\n",
      "Training loss: 1.740275001525879\n",
      "Training loss: 1.7403718948364257\n",
      "Training loss: 1.7401002883911132\n",
      "Training loss: 1.7401885986328125\n",
      "Training loss: 1.7399152755737304\n",
      "Training loss: 1.740013885498047\n",
      "Training loss: 1.7397369384765624\n",
      "Training loss: 1.7398321151733398\n",
      "Training loss: 1.7395618438720704\n",
      "Training loss: 1.7396553039550782\n",
      "Training loss: 1.7393815994262696\n",
      "Training loss: 1.7394786834716798\n",
      "Training loss: 1.7392057418823241\n",
      "Training loss: 1.7392990112304687\n",
      "Training loss: 1.7390331268310546\n",
      "Training loss: 1.7391239166259767\n",
      "Training loss: 1.7388629913330078\n",
      "Training loss: 1.7389469146728516\n",
      "Training loss: 1.7386791229248046\n",
      "Training loss: 1.7387666702270508\n",
      "Training loss: 1.7385063171386719\n",
      "Training loss: 1.7385868072509765\n",
      "Training loss: 1.7383325576782227\n",
      "Training loss: 1.7384159088134765\n",
      "Training loss: 1.7381561279296875\n",
      "Training loss: 1.7382442474365234\n",
      "Training loss: 1.7379825592041016\n",
      "Training loss: 1.7380664825439454\n",
      "Training loss: 1.7378116607666017\n",
      "Training loss: 1.7378957748413086\n",
      "Training loss: 1.7376432418823242\n",
      "Training loss: 1.7377233505249023\n",
      "Training loss: 1.7374710083007812\n",
      "Training loss: 1.7375545501708984\n",
      "Training loss: 1.7373035430908204\n",
      "Training loss: 1.7373794555664062\n",
      "Training loss: 1.7371301651000977\n",
      "Training loss: 1.7372182846069335\n",
      "Training loss: 1.7369720458984375\n",
      "Training loss: 1.7370594024658204\n",
      "Training loss: 1.7368038177490235\n",
      "Training loss: 1.7368885040283204\n",
      "Training loss: 1.7366413116455077\n",
      "Training loss: 1.736720085144043\n",
      "Training loss: 1.7364772796630858\n",
      "Training loss: 1.736547088623047\n",
      "Training loss: 1.736313819885254\n",
      "Training loss: 1.736393928527832\n",
      "Training loss: 1.7361516952514648\n",
      "Training loss: 1.7362285614013673\n",
      "Training loss: 1.735994529724121\n",
      "Training loss: 1.7360700607299804\n",
      "Training loss: 1.7358394622802735\n",
      "Training loss: 1.735916519165039\n",
      "Training loss: 1.735683059692383\n",
      "Training loss: 1.7357559204101562\n",
      "Training loss: 1.7355266571044923\n",
      "Training loss: 1.7355949401855468\n",
      "Training loss: 1.7353635787963868\n",
      "Training loss: 1.7354326248168945\n",
      "Training loss: 1.7352033615112306\n",
      "Training loss: 1.7352746963500976\n",
      "Training loss: 1.7350519180297852\n",
      "Training loss: 1.735123062133789\n",
      "Training loss: 1.7348953247070313\n",
      "Training loss: 1.7349662780761719\n",
      "Training loss: 1.734743881225586\n",
      "Training loss: 1.7348140716552733\n",
      "Training loss: 1.7345932006835938\n",
      "Training loss: 1.734658432006836\n",
      "Training loss: 1.7344326019287108\n",
      "Training loss: 1.7345035552978516\n",
      "Training loss: 1.7342815399169922\n",
      "Training loss: 1.7343458175659179\n",
      "Training loss: 1.734121322631836\n",
      "Training loss: 1.734183120727539\n",
      "Training loss: 1.733969497680664\n",
      "Training loss: 1.7340341567993165\n",
      "Training loss: 1.7338138580322267\n",
      "Training loss: 1.7338821411132812\n",
      "Training loss: 1.7336614608764649\n",
      "Training loss: 1.7337261199951173\n",
      "Training loss: 1.7335098266601563\n",
      "Training loss: 1.73358154296875\n",
      "Training loss: 1.733359146118164\n",
      "Training loss: 1.7334274291992187\n",
      "Training loss: 1.7332115173339844\n",
      "Training loss: 1.7332767486572265\n",
      "Training loss: 1.733060073852539\n",
      "Training loss: 1.7331268310546875\n",
      "Training loss: 1.7329166412353516\n",
      "Training loss: 1.7329824447631836\n",
      "Training loss: 1.7327751159667968\n",
      "Training loss: 1.7328371047973632\n",
      "Training loss: 1.7326284408569337\n",
      "Training loss: 1.7326925277709961\n",
      "Training loss: 1.7324863433837892\n",
      "Training loss: 1.7325525283813477\n",
      "Training loss: 1.7323480606079102\n",
      "Training loss: 1.732406997680664\n",
      "Training loss: 1.7322025299072266\n",
      "Training loss: 1.732257080078125\n",
      "Training loss: 1.7320636749267577\n",
      "Training loss: 1.732119369506836\n",
      "Training loss: 1.7319183349609375\n",
      "Training loss: 1.7319725036621094\n",
      "Training loss: 1.7317798614501954\n",
      "Training loss: 1.731833267211914\n",
      "Training loss: 1.7316362380981445\n",
      "Training loss: 1.7316965103149413\n",
      "Training loss: 1.731497573852539\n",
      "Training loss: 1.7315540313720703\n",
      "Training loss: 1.7313587188720703\n",
      "Training loss: 1.731416893005371\n",
      "Training loss: 1.7312236785888673\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 1.7312732696533204\n",
      "Training loss: 1.731083297729492\n",
      "Training loss: 1.731137466430664\n",
      "Training loss: 1.7309497833251952\n",
      "Training loss: 1.7309972763061523\n",
      "Training loss: 1.7308063507080078\n",
      "Training loss: 1.730865478515625\n",
      "Training loss: 1.7306758880615234\n",
      "Training loss: 1.7307283401489257\n",
      "Training loss: 1.730538558959961\n",
      "Training loss: 1.7305879592895508\n",
      "Training loss: 1.730398941040039\n",
      "Training loss: 1.7304494857788086\n",
      "Training loss: 1.7302677154541015\n",
      "Training loss: 1.7303079605102538\n",
      "Training loss: 1.730126953125\n",
      "Training loss: 1.730177879333496\n",
      "Training loss: 1.7299972534179688\n",
      "Training loss: 1.7300445556640625\n",
      "Training loss: 1.7298648834228516\n",
      "Training loss: 1.7299062728881835\n",
      "Training loss: 1.7297235488891602\n",
      "Training loss: 1.7297740936279298\n",
      "Training loss: 1.7295948028564454\n",
      "Training loss: 1.7296455383300782\n",
      "Training loss: 1.7294662475585938\n",
      "Training loss: 1.7295137405395509\n",
      "Training loss: 1.7293399810791015\n",
      "Training loss: 1.7293912887573242\n",
      "Training loss: 1.72921142578125\n",
      "Training loss: 1.7292570114135741\n",
      "Training loss: 1.729086685180664\n",
      "Training loss: 1.7291324615478516\n",
      "Training loss: 1.7289569854736329\n",
      "Training loss: 1.72900447845459\n",
      "Training loss: 1.7288307189941405\n",
      "Training loss: 1.728877639770508\n",
      "Training loss: 1.7287054061889648\n",
      "Training loss: 1.728746223449707\n",
      "Training loss: 1.7285816192626953\n",
      "Training loss: 1.7286262512207031\n",
      "Training loss: 1.728460693359375\n",
      "Training loss: 1.728499412536621\n",
      "Training loss: 1.7283353805541992\n",
      "Training loss: 1.728377342224121\n",
      "Training loss: 1.7282093048095704\n",
      "Training loss: 1.7282529830932618\n",
      "Training loss: 1.7280895233154296\n",
      "Training loss: 1.7281288146972655\n",
      "Training loss: 1.7279659271240235\n",
      "Training loss: 1.7280088424682618\n",
      "Training loss: 1.7278438568115235\n",
      "Training loss: 1.7278818130493163\n",
      "Training loss: 1.72772159576416\n",
      "Training loss: 1.7277608871459962\n",
      "Training loss: 1.7276037216186524\n",
      "Training loss: 1.7276390075683594\n",
      "Training loss: 1.7274810791015625\n",
      "Training loss: 1.7275222778320312\n",
      "Training loss: 1.7273658752441405\n",
      "Training loss: 1.7274030685424804\n",
      "Training loss: 1.7272443771362305\n",
      "Training loss: 1.7272825241088867\n"
     ]
    }
   ],
   "source": [
    "model = nn.Sequential(nn.Linear(4, 3),\n",
    "                      nn.Linear(3,2),\n",
    "                      nn.Linear(2, 1))\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.0001)\n",
    "\n",
    "x = []\n",
    "y = []\n",
    "epochs = 4000\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    optimizer.zero_grad()\n",
    "    output = model(x_train.float())\n",
    "    loss = criterion(output, y_train.float())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    running_loss += loss.item()\n",
    "    print(f\"Training loss: {running_loss/len(x_train)}\")\n",
    "    if(e % 250 == 0):\n",
    "        x.append(e)\n",
    "        y.append(running_loss/len(x_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 250, 500, 750, 1000, 1250, 1500, 1750, 2000, 2250, 2500, 2750, 3000, 3250, 3500, 3750]\n",
      "[10941.43125, 172.2093505859375, 106.01292724609375, 62.88790283203125, 36.572653198242186, 21.203897094726564, 12.476461029052734, 7.608653259277344, 4.923435211181641, 3.4534576416015623, 2.6522409439086916, 2.2172321319580077, 1.9813793182373047, 1.8536413192749024, 1.7846622467041016, 1.7473834991455077]\n"
     ]
    }
   ],
   "source": [
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt \n",
    "\n",
    "mpl.rcParams['text.color'] = 'yellow'\n",
    "plt.title(\"How loss is decreasing with epocs\") \n",
    "plt.xlabel(\"epocs\") \n",
    "plt.ylabel(\"loss\") \n",
    "plt.plot(x,y) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[418.7621],\n",
      "        [515.0216],\n",
      "        [377.7043],\n",
      "        [498.6037]])\n"
     ]
    }
   ],
   "source": [
    "x_test = torch.from_numpy(x_test)\n",
    "with torch.no_grad():\n",
    "    y_pred = model(x_test.float())\n",
    "    \n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.391883850097656"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "metrics.mean_absolute_error(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05\n",
      "0.13\n",
      "1200\n",
      "0.5\n",
      "tensor([[527.2823]])\n"
     ]
    }
   ],
   "source": [
    "a = float(input())\n",
    "b = float(input())\n",
    "c = float(input())\n",
    "d = float(input())\n",
    "\n",
    "f = np.array([[a,b,c,d]])\n",
    "f = sc.transform(f)\n",
    "f = torch.from_numpy(f)\n",
    "with torch.no_grad():\n",
    "    pred = model(f.float())\n",
    "    \n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
